<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AWS on Neil Armitage</title><link>https://neilarmitage.com/tags/aws/</link><description>Recent content in AWS on Neil Armitage</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2008–2023, Neil Armitage; all rights reserved.</copyright><lastBuildDate>Thu, 15 Dec 2022 11:32:37 +0000</lastBuildDate><atom:link href="https://neilarmitage.com/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>AWS controllers for Kubernetes</title><link>https://neilarmitage.com/post/aws_controllers_for_kubernetes/</link><pubDate>Thu, 15 Dec 2022 11:32:37 +0000</pubDate><guid>https://neilarmitage.com/post/aws_controllers_for_kubernetes/</guid><description>
I struggled to find a working (and simple) example on using ACK so I put this together to create an S3 bucket.
AWS Controllers for Kubernetes (also known as ACK) are built around the Kubernetes extension concepts of Custom Resource and Custom Resource Definitions. You can use ACK to define and use AWS services directly from Kubernetes. This helps you take advantage of managed AWS services for your Kubernetes applications without needing to define resources outside of the cluster.</description></item><item><title>SSM Bastion with no NAT Gateway</title><link>https://neilarmitage.com/post/bastion_using_ssm/</link><pubDate>Mon, 05 Dec 2022 08:43:21 +0000</pubDate><guid>https://neilarmitage.com/post/bastion_using_ssm/</guid><description>
Using Systems Manager (SSM) to control access to a Bastion host has several advantages making using a Traditional Bastion host using SSH Keys pretty much obsolete.
No need for an external IP No SSH Keys needed all access is via IAM Access logged including what command are run A working example can be found on GitHub
One of the biggest issues is that it requires access to AWS services to function so either a NAT gateway is required or VPC endpoints are needed</description></item><item><title>Terraform Testing</title><link>https://neilarmitage.com/post/terraform_testing/</link><pubDate>Fri, 02 Dec 2022 14:59:57 +0000</pubDate><guid>https://neilarmitage.com/post/terraform_testing/</guid><description>
For a long time I've been wanting to look at some way of testing terraform. As part of this I've recently looked using terratest and localstack.
While localstack looks promising lots of the things I wanted to test are either in the pro version or not supported so I went back to using live AWS accounts. It would be nice to be able to use localstack in a pipeline, hopefully in the future.</description></item><item><title>Setting up AWS Transit Gateway with Terraform</title><link>https://neilarmitage.com/post/aws_transit_gateway_with_terraform/</link><pubDate>Fri, 02 Dec 2022 12:44:02 +0000</pubDate><guid>https://neilarmitage.com/post/aws_transit_gateway_with_terraform/</guid><description>
Transit Gateway allows VPC's to be connected together into a single network as well as connecting to on-prem networks.
This example will deploy a simple setup with 2 VPC's being connected together.
The Terraform code can be found on GitHub. The code also includes a RAM (Resource Access Manager) share for linking VPC's in separate accounts but in this examples it's not used.
Just update any the reqruired settings in variables.</description></item><item><title>Deploying Nats Jetstream in AWS</title><link>https://neilarmitage.com/post/nats_jetstream_in_aws/</link><pubDate>Thu, 01 Dec 2022 12:43:36 +0000</pubDate><guid>https://neilarmitage.com/post/nats_jetstream_in_aws/</guid><description>
NATS is a connective technology that powers modern distributed systems. A connective technology is responsible for addressing, discovery and exchanging of messages that drive the common patterns in distributed systems; asking and answering questions, aka services/microservices, and making and processing statements, or stream processing.
An example installation using Terraform can be found on GitHub, this deploys a VPC and the supporting infrastructure required along with a AWS instance running Ubuntu with NAT's installed.</description></item><item><title>Setting up AWS VPN</title><link>https://neilarmitage.com/post/setting_up_aws_vpn/</link><pubDate>Thu, 01 Dec 2022 12:43:03 +0000</pubDate><guid>https://neilarmitage.com/post/setting_up_aws_vpn/</guid><description>
This example will deploy a VPN into an AWS VPC and show how to connect to it using OpenVPN from either a Mac or Linux host. It will use Certificates for authentication, many other authentication options are available.
Terraform source code can be found at Github
If will deploy a simple VPC in one AZ and then create a VPN, the required certificates for the VPN will be stored in AWS Parameter Store.</description></item><item><title>Avoiding pain when operating in the Cloud</title><link>https://neilarmitage.com/post/cloudopen-europe-2022/</link><pubDate>Wed, 14 Sep 2022 06:30:08 +0000</pubDate><guid>https://neilarmitage.com/post/cloudopen-europe-2022/</guid><description>
I will be Speaking at the Open Source Summit Europe 2022 in Dublin
The slides can be found on SpeakerDeck.</description></item><item><title>Kubernetes Backup with Velero</title><link>https://neilarmitage.com/post/k8s-backup-with-velero/</link><pubDate>Fri, 29 Jul 2022 10:14:08 +0000</pubDate><guid>https://neilarmitage.com/post/k8s-backup-with-velero/</guid><description>
Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes. It can be setup quickly with Terraform on a EKS cluster and is simple to operate.
An example deployment including EKS can be cloned from here
Installation via Terraform 1resource &amp;#34;aws_s3_bucket&amp;#34; &amp;#34;velero&amp;#34; { 2 bucket = &amp;#34;eks-velero-backup-${var.environment_name}&amp;#34; 3 acl = &amp;#34;private&amp;#34; 4 server_side_encryption_configuration { 5 rule { 6 apply_server_side_encryption_by_default { 7 sse_algorithm = &amp;#34;AES256&amp;#34; 8 } 9 } 10 } 11 versioning { 12 enabled = true 13 } 14} 15 16resource &amp;#34;aws_s3_bucket_policy&amp;#34; &amp;#34;velero&amp;#34; { 17 bucket = aws_s3_bucket.</description></item><item><title>Installing Karpenter</title><link>https://neilarmitage.com/post/karpenter/</link><pubDate>Thu, 12 May 2022 08:11:08 +0000</pubDate><guid>https://neilarmitage.com/post/karpenter/</guid><description>
Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters. It is a replacement for the Cluster Autoscaler which has some issues in AWS
Updated - 13/12/2022 : The installation is now easier with the new Terraform module - see here for an example
This at the moment this example does not work on an acloudguru sandbox account.</description></item><item><title>Basic EKS Cluster with Cluster Autoscaler</title><link>https://neilarmitage.com/post/basic-eks-cluster/</link><pubDate>Wed, 11 May 2022 10:38:08 +0000</pubDate><guid>https://neilarmitage.com/post/basic-eks-cluster/</guid><description>
EKSCTL can be used to quickly deploy a AWS EKS Cluster.
This is based on using a sandbox AWS account The supporting files can be found on Github
Create an EKS deployment file, I tend to create individual nodegroups dedicated to a single AZ
1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ cat ca-cluster.yaml 3 4--- 5apiVersion: eksctl.io/v1alpha5 6kind: ClusterConfig 7 8metadata: 9 name: ca-cluster 10--- 11apiVersion: eksctl.io/v1alpha5 12kind: ClusterConfig 13 14metadata: 15 name: ca-cluster 16 region: us-east-1 17 version: &amp;#34;1.</description></item><item><title>EKS with Cilium in chaining mode</title><link>https://neilarmitage.com/post/cilium-chaining/</link><pubDate>Wed, 11 May 2022 10:38:08 +0000</pubDate><guid>https://neilarmitage.com/post/cilium-chaining/</guid><description>
Cilium can run in chaining mode which allows it to run alongside the AWS-CNI plugin.
This is based on using a sandbox AWS account The supporting files can be found on Github
Install some extra tools
1curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} 2sha256sum --check cilium-linux-amd64.tar.gz.sha256sum 3sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin 4rm cilium-linux-amd64.tar.gz{,.sha256sum} 5 6 7export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt) 8curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum} 9sha256sum --check hubble-linux-amd64.tar.gz.sha256sum 10sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin 11rm hubble-linux-amd64.tar.gz{,.sha256sum} Deploy an EKS Cluster</description></item></channel></rss>