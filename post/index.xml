<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Neil Armitage</title><link>https://neilarmitage.com/post/</link><description>Recent content in Posts on Neil Armitage</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright © 2008–2023, Neil Armitage; all rights reserved.</copyright><lastBuildDate>Fri, 13 Jan 2023 13:57:45 +0000</lastBuildDate><atom:link href="https://neilarmitage.com/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Using kind to Test Kubernetes</title><link>https://neilarmitage.com/post/using_kind/</link><pubDate>Fri, 13 Jan 2023 13:57:45 +0000</pubDate><guid>https://neilarmitage.com/post/using_kind/</guid><description>
kind is a tool for running local Kubernetes clusters using Docker container “nodes”. kind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.
It requires Docker to be installed (it may work with podman but I've not tried) and the kind binary which can be installed in various ways
1brew install kind 2or 3sudo port selfupdate &amp;amp;&amp;amp; sudo port install kind 4or 5choco install kind 6or 7go install sigs.</description></item><item><title>Prometheus Alert Manager</title><link>https://neilarmitage.com/post/prometheus_alert_manager/</link><pubDate>Thu, 05 Jan 2023 08:41:16 +0000</pubDate><guid>https://neilarmitage.com/post/prometheus_alert_manager/</guid><description>
The Prometheus Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.
To ensure that alert messages get delivered it's a good idea to run several alertmanager processes clustered together, so in this example we will deploy 2 on different machines</description></item><item><title>Monitoring NGINX with Prometheus</title><link>https://neilarmitage.com/post/prometheus_nginx/</link><pubDate>Tue, 03 Jan 2023 08:38:26 +0000</pubDate><guid>https://neilarmitage.com/post/prometheus_nginx/</guid><description>
Using the NGINX Prometheus exporter to add MGINX metrics into Prometheus.
NGINX Prometheus exporter fetches the metrics from a single NGINX or NGINX Plus, converts the metrics into appropriate Prometheus metrics types and finally exposes them via an HTTP server to be collected by Prometheus.
These instructions assume you already have a running Prometheus server - see here for an example.
Installing NGINX and exposing metrics Using a Ubuntu host</description></item><item><title>Installing Prometheus</title><link>https://neilarmitage.com/post/prometheus_install/</link><pubDate>Mon, 02 Jan 2023 08:40:51 +0000</pubDate><guid>https://neilarmitage.com/post/prometheus_install/</guid><description>
Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. It has been used by many companies for over 10 years and forms the foundation of a good and flexiable monitoring system
In this the first of a series of posts we will go back to the basics installing Prometheus and node exporter on either VM's or Bare-metal hosts. In later posts the installation on Kubernetes will be covered.</description></item><item><title>AWS controllers for Kubernetes</title><link>https://neilarmitage.com/post/aws_controllers_for_kubernetes/</link><pubDate>Thu, 15 Dec 2022 11:32:37 +0000</pubDate><guid>https://neilarmitage.com/post/aws_controllers_for_kubernetes/</guid><description>
I struggled to find a working (and simple) example on using ACK so I put this together to create an S3 bucket.
AWS Controllers for Kubernetes (also known as ACK) are built around the Kubernetes extension concepts of Custom Resource and Custom Resource Definitions. You can use ACK to define and use AWS services directly from Kubernetes. This helps you take advantage of managed AWS services for your Kubernetes applications without needing to define resources outside of the cluster.</description></item><item><title>Upgrade Kubernetes to 1.26</title><link>https://neilarmitage.com/post/upgrade_kubernetes/</link><pubDate>Mon, 12 Dec 2022 13:05:25 +0000</pubDate><guid>https://neilarmitage.com/post/upgrade_kubernetes/</guid><description>
The following will update the 3 node cluster build here to 1.26.
Before installing 1.26 the hosts need to be running containerd &amp;gt; 1.6, the Ubuntu 20.04 hosts can be upgraded using the instructions here
Upgrading the Master node 1$ export RELEASE=1.26.0 2 3$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y --allow-change-held-packages kubeadm=$RELEASE-00 4 5$ kubectl drain k8s-control --ignore-daemonsets 6 7$ sudo kubeadm upgrade plan v$RELEASE 8 9[upgrade/config] Making sure the configuration is correct: 10[upgrade/config] Reading configuration from the cluster.</description></item><item><title>Upgrade Containerd on Ubuntu 20.04</title><link>https://neilarmitage.com/post/upgrade_containerd/</link><pubDate>Mon, 12 Dec 2022 13:05:14 +0000</pubDate><guid>https://neilarmitage.com/post/upgrade_containerd/</guid><description>
Kubernetes 1.26 requires Containerd &amp;gt; 1.6 but the highest version in the Ubuntu 20.04 repos is 1.5.x. The following instructions will get a 20.04 host ready to upgrade to Kubernetes 1.26.
The following instructions assume you are running as root
1 2mkdir -p /etc/apt/keyrings 3curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg 4 5echo \ 6 &amp;#34;deb [arch=$(dpkg --print-architecture) \ 7 signed-by=/etc/apt/keyrings/docker.gpg] \ 8 https://download.docker.com/linux/ubuntu \ 9 $(lsb_release -cs) stable&amp;#34; | \ 10 sudo tee /etc/apt/sources.</description></item><item><title>Building a basic kubernetes cluster with kubeadm</title><link>https://neilarmitage.com/post/basic_kubernetes_cluster/</link><pubDate>Fri, 09 Dec 2022 14:07:31 +0000</pubDate><guid>https://neilarmitage.com/post/basic_kubernetes_cluster/</guid><description>
This is just a basic setup installing a 3 node Kubernetes setup on 3 nodes. The nodes can probably be anywhere, AWS, GCP, VMware etc as long as they are running Ubuntu 20.04.
One node needs to be designated as the master and the other 2 as workers.
To start with the pre-requisites need to be installed on all 3 nodes. This can be done via a script or by entering the commands below individually</description></item><item><title>SSM Bastion with no NAT Gateway</title><link>https://neilarmitage.com/post/bastion_using_ssm/</link><pubDate>Mon, 05 Dec 2022 08:43:21 +0000</pubDate><guid>https://neilarmitage.com/post/bastion_using_ssm/</guid><description>
Using Systems Manager (SSM) to control access to a Bastion host has several advantages making using a Traditional Bastion host using SSH Keys pretty much obsolete.
No need for an external IP No SSH Keys needed all access is via IAM Access logged including what command are run A working example can be found on GitHub
One of the biggest issues is that it requires access to AWS services to function so either a NAT gateway is required or VPC endpoints are needed</description></item><item><title>Terraform Testing</title><link>https://neilarmitage.com/post/terraform_testing/</link><pubDate>Fri, 02 Dec 2022 14:59:57 +0000</pubDate><guid>https://neilarmitage.com/post/terraform_testing/</guid><description>
For a long time I've been wanting to look at some way of testing terraform. As part of this I've recently looked using terratest and localstack.
While localstack looks promising lots of the things I wanted to test are either in the pro version or not supported so I went back to using live AWS accounts. It would be nice to be able to use localstack in a pipeline, hopefully in the future.</description></item><item><title>Setting up AWS Transit Gateway with Terraform</title><link>https://neilarmitage.com/post/aws_transit_gateway_with_terraform/</link><pubDate>Fri, 02 Dec 2022 12:44:02 +0000</pubDate><guid>https://neilarmitage.com/post/aws_transit_gateway_with_terraform/</guid><description>
Transit Gateway allows VPC's to be connected together into a single network as well as connecting to on-prem networks.
This example will deploy a simple setup with 2 VPC's being connected together.
The Terraform code can be found on GitHub. The code also includes a RAM (Resource Access Manager) share for linking VPC's in separate accounts but in this examples it's not used.
Just update any the reqruired settings in variables.</description></item><item><title>Deploying Nats Jetstream in AWS</title><link>https://neilarmitage.com/post/nats_jetstream_in_aws/</link><pubDate>Thu, 01 Dec 2022 12:43:36 +0000</pubDate><guid>https://neilarmitage.com/post/nats_jetstream_in_aws/</guid><description>
NATS is a connective technology that powers modern distributed systems. A connective technology is responsible for addressing, discovery and exchanging of messages that drive the common patterns in distributed systems; asking and answering questions, aka services/microservices, and making and processing statements, or stream processing.
An example installation using Terraform can be found on GitHub, this deploys a VPC and the supporting infrastructure required along with a AWS instance running Ubuntu with NAT's installed.</description></item><item><title>Setting up AWS VPN</title><link>https://neilarmitage.com/post/setting_up_aws_vpn/</link><pubDate>Thu, 01 Dec 2022 12:43:03 +0000</pubDate><guid>https://neilarmitage.com/post/setting_up_aws_vpn/</guid><description>
This example will deploy a VPN into an AWS VPC and show how to connect to it using OpenVPN from either a Mac or Linux host. It will use Certificates for authentication, many other authentication options are available.
Terraform source code can be found at Github
If will deploy a simple VPC in one AZ and then create a VPN, the required certificates for the VPN will be stored in AWS Parameter Store.</description></item><item><title>Avoiding pain when operating in the Cloud</title><link>https://neilarmitage.com/post/cloudopen-europe-2022/</link><pubDate>Wed, 14 Sep 2022 06:30:08 +0000</pubDate><guid>https://neilarmitage.com/post/cloudopen-europe-2022/</guid><description>
I will be Speaking at the Open Source Summit Europe 2022 in Dublin
The slides can be found on SpeakerDeck.</description></item><item><title>Kubernetes Backup with Velero</title><link>https://neilarmitage.com/post/k8s-backup-with-velero/</link><pubDate>Fri, 29 Jul 2022 10:14:08 +0000</pubDate><guid>https://neilarmitage.com/post/k8s-backup-with-velero/</guid><description>
Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes. It can be setup quickly with Terraform on a EKS cluster and is simple to operate.
An example deployment including EKS can be cloned from here
Installation via Terraform 1resource &amp;#34;aws_s3_bucket&amp;#34; &amp;#34;velero&amp;#34; { 2 bucket = &amp;#34;eks-velero-backup-${var.environment_name}&amp;#34; 3 acl = &amp;#34;private&amp;#34; 4 server_side_encryption_configuration { 5 rule { 6 apply_server_side_encryption_by_default { 7 sse_algorithm = &amp;#34;AES256&amp;#34; 8 } 9 } 10 } 11 versioning { 12 enabled = true 13 } 14} 15 16resource &amp;#34;aws_s3_bucket_policy&amp;#34; &amp;#34;velero&amp;#34; { 17 bucket = aws_s3_bucket.</description></item><item><title>Installing Karpenter</title><link>https://neilarmitage.com/post/karpenter/</link><pubDate>Thu, 12 May 2022 08:11:08 +0000</pubDate><guid>https://neilarmitage.com/post/karpenter/</guid><description>
Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters. It is a replacement for the Cluster Autoscaler which has some issues in AWS
Updated - 13/12/2022 : The installation is now easier with the new Terraform module - see here for an example
This at the moment this example does not work on an acloudguru sandbox account.</description></item><item><title>Basic EKS Cluster with Cluster Autoscaler</title><link>https://neilarmitage.com/post/basic-eks-cluster/</link><pubDate>Wed, 11 May 2022 10:38:08 +0000</pubDate><guid>https://neilarmitage.com/post/basic-eks-cluster/</guid><description>
EKSCTL can be used to quickly deploy a AWS EKS Cluster.
This is based on using a sandbox AWS account The supporting files can be found on Github
Create an EKS deployment file, I tend to create individual nodegroups dedicated to a single AZ
1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ cat ca-cluster.yaml 3 4--- 5apiVersion: eksctl.io/v1alpha5 6kind: ClusterConfig 7 8metadata: 9 name: ca-cluster 10--- 11apiVersion: eksctl.io/v1alpha5 12kind: ClusterConfig 13 14metadata: 15 name: ca-cluster 16 region: us-east-1 17 version: &amp;#34;1.</description></item><item><title>EKS with Cilium in chaining mode</title><link>https://neilarmitage.com/post/cilium-chaining/</link><pubDate>Wed, 11 May 2022 10:38:08 +0000</pubDate><guid>https://neilarmitage.com/post/cilium-chaining/</guid><description>
Cilium can run in chaining mode which allows it to run alongside the AWS-CNI plugin.
This is based on using a sandbox AWS account The supporting files can be found on Github
Install some extra tools
1curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} 2sha256sum --check cilium-linux-amd64.tar.gz.sha256sum 3sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin 4rm cilium-linux-amd64.tar.gz{,.sha256sum} 5 6 7export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt) 8curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum} 9sha256sum --check hubble-linux-amd64.tar.gz.sha256sum 10sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin 11rm hubble-linux-amd64.tar.gz{,.sha256sum} Deploy an EKS Cluster</description></item><item><title>Using ACloudGuru AWS Sandboxes</title><link>https://neilarmitage.com/post/acloudguru-sandbox/</link><pubDate>Sun, 08 May 2022 14:38:08 +0000</pubDate><guid>https://neilarmitage.com/post/acloudguru-sandbox/</guid><description>
Running your own AWS account for testing can lead to unexpected costs. Unless care is taken around securing the account, the account can be hijacked and used for other purposes. Personally I’ve stopped running my own accounts and moved to using the sandbox accounts provided by acloudguru as part of their Personal Plus subscription.
Most of the examples provided in the blog should run on these playgrounds with some exceptions. Generally the setup is using the AWS CloudShell within the sandbox account to remove any issue with local machine setups.</description></item></channel></rss>