[{"body":"","link":"https://neilarmitage.com/","section":"","tags":null,"title":""},{"body":"","link":"https://neilarmitage.com/tags/ack/","section":"tags","tags":null,"title":"ACK"},{"body":"","link":"https://neilarmitage.com/tags/aws/","section":"tags","tags":null,"title":"AWS"},{"body":"I struggled to find a working (and simple) example on using ACK so I put this together to create an S3 bucket.\nAWS Controllers for Kubernetes (also known as ACK) are built around the Kubernetes extension concepts of Custom Resource and Custom Resource Definitions. You can use ACK to define and use AWS services directly from Kubernetes. This helps you take advantage of managed AWS services for your Kubernetes applications without needing to define resources outside of the cluster.\nSay you need to use a AWS S3 Bucket in your application thatâ€™s deployed to Kubernetes. Instead of using AWS console, AWS CLI, AWS CloudFormation etc., you can define the AWS S3 Bucket in a YAML manifest file and deploy it using familiar tools such as kubectl. The end goal is to allow users (Software Engineers, DevOps engineers, operators etc.) to use the same interface (Kubernetes API in this case) to describe and manage AWS services along with native Kubernetes resources such as Deployment, Service etc.\nDeploy a test cluster with EKS\n1 2$ cat cluster.yaml 3 4apiVersion: eksctl.io/v1alpha5 5kind: ClusterConfig 6metadata: 7 name: ack-cluster 8 region: us-east-1 9 version: \u0026#34;1.23\u0026#34; 10iam: 11 withOIDC: true 12 13managedNodeGroups: 14 - name: managed-ng-1 15 minSize: 1 16 maxSize: 2 17 desiredCapacity: 1 18 instanceType: t3.small 19 spot: true 20 21 22$ eksctl create cluster -f cluster.yml Install the S3 controller\n1export SERVICE=s3 2export RELEASE_VERSION=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4` 3export ACK_SYSTEM_NAMESPACE=ack-system 4export AWS_REGION=us-east-1 5 6aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws 7 8helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller \\ 9 oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION --set=aws.region=$AWS_REGION 10 11$ kubectl get crd 12 13NAME CREATED AT 14buckets.s3.services.k8s.aws 2022-12-15T11:05:35Z Add the IAM Permissions to the POD\n1export SERVICE=s3 2export ACK_K8S_SERVICE_ACCOUNT_NAME=ack-$SERVICE-controller 3 4export ACK_SYSTEM_NAMESPACE=ack-system 5export EKS_CLUSTER_NAME=ack-cluster 6export POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess 7 8# IAM role has a format - do not change it. you can\u0026#39;t use any arbitrary name 9export IAM_ROLE_NAME=ack-$SERVICE-controller-role 10 11eksctl create iamserviceaccount \\ 12 --name $ACK_K8S_SERVICE_ACCOUNT_NAME \\ 13 --namespace $ACK_SYSTEM_NAMESPACE \\ 14 --cluster $EKS_CLUSTER_NAME \\ 15 --role-name $IAM_ROLE_NAME \\ 16 --attach-policy-arn $POLICY_ARN \\ 17 --approve \\ 18 --override-existing-serviceaccounts and restart the controller pod to get the new IAM permissions\n1$ kubectl get deployments -n $ACK_SYSTEM_NAMESPACE 2 3NAME READY UP-TO-DATE AVAILABLE AGE 4ack-s3-controller-s3-chart 1/1 1 1 7m1s 5 6$ kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deployment ack-s3-controller-s3-chart 7deployment.apps/ack-s3-controller-s3-chart restarted 8 9$ kubectl get pods -n $ACK_SYSTEM_NAMESPACE 10NAME READY STATUS RESTARTS AGE 11ack-s3-controller-s3-chart-87c8bbfbc-fjjfk 1/1 Running 0 5m33s 12 13$ kubectl describe pod -n $ACK_SYSTEM_NAMESPACE ack-s3-controller-s3-chart-87c8bbfbc-fjjfk | grep \u0026#34;^\\s*AWS_\u0026#34; 14 AWS_REGION: us-east-1 15 AWS_ENDPOINT_URL: 16 AWS_STS_REGIONAL_ENDPOINTS: regional 17 AWS_ROLE_ARN: arn:aws:iam::484235524795:role/ack-s3-controller-role 18 AWS_WEB_IDENTITY_TOKEN_FILE: /var/run/secrets/eks.amazonaws.com/serviceaccount/token Deploy the S3 bucket\n1 2$ cat bucket.yml 3apiVersion: s3.services.k8s.aws/v1alpha1 4kind: Bucket 5metadata: 6 name: my-ack-s3-bucket-123456789 7spec: 8 name: my-ack-s3-bucket-123456789 9 tagging: 10 tagSet: 11 - key: myTagKey 12 value: myTagValue 13 14$ kubectl apply -f bucket.yml 15 16$ kubectl get bucket 17NAME AGE 18my-ack-s3-bucket-123456789 26m 19 20$ kubectl describe bucket my-ack-s3-bucket-123456789 21Name: my-ack-s3-bucket-123456789 22Namespace: default 23Labels: \u0026lt;none\u0026gt; 24Annotations: \u0026lt;none\u0026gt; 25API Version: s3.services.k8s.aws/v1alpha1 26Kind: Bucket 27Metadata: 28 Creation Timestamp: 2022-12-15T11:19:20Z 29 Finalizers: 30 finalizers.s3.services.k8s.aws/Bucket 31 Generation: 1 32 Managed Fields: 33 API Version: s3.services.k8s.aws/v1alpha1 34 Fields Type: FieldsV1 35 fieldsV1: 36 f:metadata: 37 f:annotations: 38 .: 39 f:kubectl.kubernetes.io/last-applied-configuration: 40 f:spec: 41 .: 42 f:name: 43 f:tagging: 44 .: 45 f:tagSet: 46 Manager: kubectl-client-side-apply 47 Operation: Update 48 Time: 2022-12-15T11:19:20Z 49 API Version: s3.services.k8s.aws/v1alpha1 50 Fields Type: FieldsV1 51 fieldsV1: 52 f:metadata: 53 f:finalizers: 54 .: 55 v:\u0026#34;finalizers.s3.services.k8s.aws/Bucket\u0026#34;: 56 Manager: controller 57 Operation: Update 58 Time: 2022-12-15T11:19:21Z 59 API Version: s3.services.k8s.aws/v1alpha1 60 Fields Type: FieldsV1 61 fieldsV1: 62 f:status: 63 .: 64 f:ackResourceMetadata: 65 .: 66 f:ownerAccountID: 67 f:region: 68 f:conditions: 69 f:location: 70 Manager: controller 71 Operation: Update 72 Subresource: status 73 Time: 2022-12-15T11:19:21Z 74 Resource Version: 8095 75 UID: d973c580-d5ba-4a8c-a1cc-fdb3e2128f6d 76Spec: 77 Name: my-ack-s3-bucket-123456789 78 Tagging: 79 Tag Set: 80 Key: myTagKey 81 Value: myTagValue 82Status: 83 Ack Resource Metadata: 84 Owner Account ID: 484235524795 85 Region: us-east-1 86 Conditions: 87 Last Transition Time: 2022-12-15T11:19:21Z 88 Message: Resource synced successfully 89 Reason: 90 Status: True 91 Type: ACK.ResourceSynced 92 Location: /my-ack-s3-bucket-123456789 93Events: \u0026lt;none\u0026gt; Delete the bucket and the cluster\n1 2kubectl delete -f bucket.yml 3 4exkctl delete cluster -f cluster.yml ","link":"https://neilarmitage.com/post/aws_controllers_for_kubernetes/","section":"post","tags":["kubernetes","AWS","ACK"],"title":"AWS controllers for Kubernetes"},{"body":"","link":"https://neilarmitage.com/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"","link":"https://neilarmitage.com/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://neilarmitage.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"The following will update the 3 node cluster build here to 1.26.\nBefore installing 1.26 the hosts need to be running containerd \u0026gt; 1.6, the Ubuntu 20.04 hosts can be upgraded using the instructions here\nUpgrading the Master node 1$ export RELEASE=1.26.0 2 3$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y --allow-change-held-packages kubeadm=$RELEASE-00 4 5$ kubectl drain k8s-control --ignore-daemonsets 6 7$ sudo kubeadm upgrade plan v$RELEASE 8 9[upgrade/config] Making sure the configuration is correct: 10[upgrade/config] Reading configuration from the cluster... 11[upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; 12[preflight] Running pre-flight checks. 13[upgrade] Running cluster health checks 14[upgrade] Fetching available versions to upgrade to 15[upgrade/versions] Cluster version: v1.25.3 16[upgrade/versions] kubeadm version: v1.26.0 17[upgrade/versions] Target version: v1.26.0 18[upgrade/versions] Latest version in the v1.25 series: v1.26.0 19 20W1212 10:19:42.929073 4860 configset.go:177] error unmarshaling configuration schema.GroupVersionKind{Group:\u0026#34;kubeproxy.config.k8s.io\u0026#34;, Version:\u0026#34;v1alpha1\u0026#34;, Kind:\u0026#34;KubeProxyConfiguration\u0026#34;}: strict decoding error: unknown field \u0026#34;udpIdleTimeout\u0026#34; 21Components that must be upgraded manually after you have upgraded the control plane with \u0026#39;kubeadm upgrade apply\u0026#39;: 22COMPONENT CURRENT TARGET 23kubelet 3 x v1.25.4 v1.26.0 24 25Upgrade to the latest version in the v1.25 series: 26 27COMPONENT CURRENT TARGET 28kube-apiserver v1.25.3 v1.26.0 29kube-controller-manager v1.25.3 v1.26.0 30kube-scheduler v1.25.3 v1.26.0 31kube-proxy v1.25.3 v1.26.0 32CoreDNS v1.9.3 v1.9.3 33etcd 3.5.4-0 3.5.6-0 34 35You can now apply the upgrade by executing the following command: 36 37\tkubeadm upgrade apply v1.26.0 38 39_____________________________________________________________________ 40 41 42The table below shows the current state of component configs as understood by this version of kubeadm. 43Configs that have a \u0026#34;yes\u0026#34; mark in the \u0026#34;MANUAL UPGRADE REQUIRED\u0026#34; column require manual config upgrade or 44resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually 45upgrade to is denoted in the \u0026#34;PREFERRED VERSION\u0026#34; column. 46 47API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED 48kubeproxy.config.k8s.io v1alpha1 v1alpha1 no 49kubelet.config.k8s.io v1beta1 v1beta1 no 50 51 52 53$ sudo kubeadm upgrade apply v$RELEASE 54 55[upgrade/config] Making sure the configuration is correct: 56[upgrade/config] Reading configuration from the cluster... 57[upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; 58W1212 10:20:27.629919 5162 configset.go:177] error unmarshaling configuration schema.GroupVersionKind{Group:\u0026#34;kubeproxy.config.k8s.io\u0026#34;, Version:\u0026#34;v1alpha1\u0026#34;, Kind:\u0026#34;KubeProxyConfiguration\u0026#34;}: strict decoding error: unknown field \u0026#34;udpIdleTimeout\u0026#34; 59[preflight] Running pre-flight checks. 60[upgrade] Running cluster health checks 61[upgrade/version] You have chosen to change the cluster version to \u0026#34;v1.26.0\u0026#34; 62[upgrade/versions] Cluster version: v1.25.3 63[upgrade/versions] kubeadm version: v1.26.0 64[upgrade] Are you sure you want to proceed? [y/N]: y 65[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster 66[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection 67[upgrade/prepull] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; 68[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026#34;v1.26.0\u0026#34; (timeout: 5m0s).. 69...... 70[addons] Applied essential addon: CoreDNS 71[addons] Applied essential addon: kube-proxy 72 73[upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.26.0\u0026#34;. Enjoy! 74 75[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so. 76 77$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y --allow-change-held-packages kubelet=$RELEASE-00 kubectl=$RELEASE-00 78 79$ sudo systemctl daemon-reload 80 81$ sudo systemctl restart kubelet 82 83$ kubectl uncordon k8s-control 84 85$ kubectl get nodes Upgrading the Worker nodes Note: these need to be followed on both workers nodes\nOn the master run\n1$ kubectl drain worker-1|2 --ignore-daemonsets --force On the worker node\n1 2$ export RELEASE=1.26.0 3$ sudo apt-get update \u0026amp;\u0026amp; 4$ sudo apt-get install -y --allow-change-held-packages kubeadm=$RELEASE-00 5 6$ sudo kubeadm upgrade node 7 8[upgrade] Reading configuration from the cluster... 9[upgrade] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; 10[preflight] Running pre-flight checks 11[preflight] Skipping prepull. Not a control plane node. 12[upgrade] Skipping phase. Not a control plane node. 13[kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; 14[upgrade] The configuration for this node was successfully updated! 15[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 16 17$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y --allow-change-held-packages kubelet=$RELEASE-00 kubectl=$RELEASE-00 18 19$ sudo systemctl daemon-reload 20$ sudo systemctl restart kubelet Back on the master run\n1 2$ kubectl uncordon worker-1|2 ","link":"https://neilarmitage.com/post/upgrade_kubernetes/","section":"post","tags":["kubernetes"],"title":"Upgrade Kubernetes to 1.26"},{"body":"Kubernetes 1.26 requires Containerd \u0026gt; 1.6 but the highest version in the Ubuntu 20.04 repos is 1.5.x. The following instructions will get a 20.04 host ready to upgrade to Kubernetes 1.26.\nThe following instructions assume you are running as root\n1 2mkdir -p /etc/apt/keyrings 3curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg 4 5echo \\ 6 \u0026#34;deb [arch=$(dpkg --print-architecture) \\ 7 signed-by=/etc/apt/keyrings/docker.gpg] \\ 8 https://download.docker.com/linux/ubuntu \\ 9 $(lsb_release -cs) stable\u0026#34; | \\ 10 sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 11 12apt update 13apt install containerd.io 14 15systemctl restart containerd 16 17mv /etc/containerd/config.toml /etc/containerd/config.toml.orig 18containerd config default | sudo tee /etc/containerd/config.toml 19sed -i \u0026#39;s/SystemdCgroup \\= false/SystemdCgroup \\= true/g\u0026#39; /etc/containerd/config.toml 20 21wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz 22 23mkdir -p /opt/cni/bin 24tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz 25 26systemctl restart containerd ","link":"https://neilarmitage.com/post/upgrade_containerd/","section":"post","tags":["kubernetes"],"title":"Upgrade Containerd on Ubuntu 20.04"},{"body":" This is just a basic setup installing a 3 node Kubernetes setup on 3 nodes. The nodes can probably be anywhere, AWS, GCP, VMware etc as long as they are running Ubuntu 20.04.\nOne node needs to be designated as the master and the other 2 as workers.\nTo start with the pre-requisites need to be installed on all 3 nodes. This can be done via a script or by entering the commands below individually\nDownload and run the script\n1curl https://raw.githubusercontent.com/narmitag/kubernetes/main/basic_setup/setup_machine.sh -o setup_machine.sh 2chmod +x setup_machine.sh 3sudo ./setup_machine.sh Running the steps:\n1#Install containerd 2cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf 3overlay 4br_netfilter 5EOF 6 7sudo modprobe overlay 8sudo modprobe br_netfilter 9 10cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf 11net.bridge.bridge-nf-call-iptables = 1 12net.ipv4.ip_forward = 1 13net.bridge.bridge-nf-call-ip6tables = 1 14EOF 15 16sudo sysctl --system 17sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y containerd 18 19sudo mkdir -p /etc/containerd 20sudo containerd config default | sudo tee /etc/containerd/config.toml 21sudo systemctl restart containerd 22 23#Disable swap 24sudo swapoff -a 25sudo sed -e \u0026#39;/swap/ s/^#*/#/\u0026#39; -i /etc/fstab 26curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - 27 28#Install Kubernetes binaries 29cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 30deb https://apt.kubernetes.io/ kubernetes-xenial main 31EOF 32 33sudo apt-get update 34sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00 35sudo apt-mark hold kubelet kubeadm kubectl The cluster now needs to be bootstrapped from the master\n1$ sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0 2 3$ mkdir -p $HOME/.kube 4$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 5$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 6 7$ kubectl get no 8NAME STATUS ROLES AGE VERSION 9master NotReady control-plane 50s v1.24.0 A network plugin needs to be installed to get the node ready (it may take a few minutes for the node to become ready). This will install calico\n1$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 2 3poddisruptionbudget.policy/calico-kube-controllers created 4serviceaccount/calico-kube-controllers created 5serviceaccount/calico-node created 6configmap/calico-config created 7customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created 8customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created 9customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created 10customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created 11customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created 12customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created 13customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created 14customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created 15customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created 16customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created 17customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created 18customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created 19customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created 20customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created 21customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created 22customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created 23customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created 24clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created 25clusterrole.rbac.authorization.k8s.io/calico-node created 26clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created 27clusterrolebinding.rbac.authorization.k8s.io/calico-node created 28daemonset.apps/calico-node created 29deployment.apps/calico-kube-controllers created 30 31$ kubectl get no 32NAME STATUS ROLES AGE VERSION 33master Ready control-plane 87s v1.24.0 The workers can now be connected, so get kubeadm to generate the required command on the master.\n1$ kubeadm token create --print-join-command The run the command generated above on each worker\n1$ sudo kubeadmin join ............ The master should now show 3 nodes\n1 2$ kubectl get no 3NAME STATUS ROLES AGE VERSION 4master Ready control-plane 120s v1.24.0 5worker-1 Ready \u0026lt;none\u0026gt; 77s v1.24.0 6worker-2 Ready \u0026lt;none\u0026gt; 97s v1.24.0 ","link":"https://neilarmitage.com/post/basic_kubernetes_cluster/","section":"post","tags":["kubernetes"],"title":"Building a basic kubernetes cluster with kubeadm"},{"body":"Using Systems Manager (SSM) to control access to a Bastion host has several advantages making using a Traditional Bastion host using SSH Keys pretty much obsolete.\nNo need for an external IP No SSH Keys needed all access is via IAM Access logged including what command are run A working example can be found on GitHub\nOne of the biggest issues is that it requires access to AWS services to function so either a NAT gateway is required or VPC endpoints are needed\nThe following endpoints need to be created to allow SSM to work without a NAT Gateway\nSSM SSMMessages EC2 EC2Messages KMS Logs 1module \u0026#34;vpc_endpoints\u0026#34; { 2 source = \u0026#34;terraform-aws-modules/vpc/aws//modules/vpc-endpoints\u0026#34; 3 version = \u0026#34;3.14.2\u0026#34; 4 5 vpc_id = module.vpc.vpc_id 6 security_group_ids = [data.aws_security_group.default.id] 7 8 endpoints = { 9 ssm = { 10 service = \u0026#34;ssm\u0026#34; 11 private_dns_enabled = true 12 subnet_ids = module.vpc.private_subnets 13 security_group_ids = [aws_security_group.vpc_tls.id] 14 }, 15 ssmmessages = { 16 service = \u0026#34;ssmmessages\u0026#34; 17 private_dns_enabled = true 18 subnet_ids = module.vpc.private_subnets 19 security_group_ids = [aws_security_group.vpc_tls.id] 20 }, 21 ec2 = { 22 service = \u0026#34;ec2\u0026#34; 23 private_dns_enabled = true 24 subnet_ids = module.vpc.private_subnets 25 security_group_ids = [data.aws_security_group.default.id] 26 }, 27 ec2messages = { 28 service = \u0026#34;ec2messages\u0026#34; 29 private_dns_enabled = true 30 subnet_ids = module.vpc.private_subnets 31 security_group_ids = [aws_security_group.vpc_tls.id] 32 }, 33 kms = { 34 service = \u0026#34;kms\u0026#34; 35 private_dns_enabled = true 36 subnet_ids = module.vpc.private_subnets 37 security_group_ids = [aws_security_group.vpc_tls.id] 38 }, 39 logs = { 40 service = \u0026#34;logs\u0026#34; 41 private_dns_enabled = true 42 subnet_ids = module.vpc.private_subnets 43 security_group_ids = [aws_security_group.vpc_tls.id] 44 }, 45 } 46 47} The instance deployed is just a standard AWS AL2 instance with the IAM manged role arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore attached to it via an instance profile.\nThe Terraform example here includes things like logging and KMS encryption which could be removed. All the command run on the bastion are recorded in Cloudwatch logs along with the name of the IAM user who ran them.\nTo connect to the host there is a bash script in client\\ssh_terminal which will locate the bastion in AWS and start a session to it. Basically it is running the command\n1aws ssm start-session --target \u0026#34;${INSTANCE_ID}\u0026#34; --region \u0026#34;${REGION}\u0026#34; The instance can also be used for Port forwarding, this is a bit hacky but works. This will allow connection from your local machine to a remote host via the bastion.\nStep 1: Start an SSH session to the bastion and run\n1socat TCP-LISTEN:4222,reuseaddr,fork TCP4:10.200.36.194:4222 Step 2: On your local machine start a port forwarding SSM Session\n1 aws ssm start-session --target \u0026#34;${INSTANCE_ID}\u0026#34; \\ 2 --document-name AWS-StartPortForwardingSession \\ 3 --parameters \u0026#39;{\u0026#34;portNumber\u0026#34;:[\u0026#34;4222\u0026#34;],\u0026#34;localPortNumber\u0026#34;:[\u0026#34;9999\u0026#34;]}\u0026#39; Step 3: Connect to port 9999 locally which will forward via the Bastion to port 4333 on 10.200.36.194\n","link":"https://neilarmitage.com/post/bastion_using_ssm/","section":"post","tags":["AWS"],"title":"SSM Bastion with no NAT Gateway"},{"body":"","link":"https://neilarmitage.com/tags/terraform/","section":"tags","tags":null,"title":"terraform"},{"body":"For a long time I've been wanting to look at some way of testing terraform. As part of this I've recently looked using terratest and localstack.\nWhile localstack looks promising lots of the things I wanted to test are either in the pro version or not supported so I went back to using live AWS accounts. It would be nice to be able to use localstack in a pipeline, hopefully in the future. I did get a simple pipeline running in GitHub Actions\n1name: localstack-action-example 2on: push 3jobs: 4 example-job: 5 runs-on: ubuntu-latest 6 steps: 7 - name: Checkout 8 uses: actions/checkout@v3 9 - name: Start LocalStack 10 env: 11 LOCALSTACK_API_KEY: ${{ secrets.LOCALSTACK_API_KEY }} 12 run: | 13 pip install --upgrade pyopenssl 14 pip install localstack awscli-local[ver1] # install LocalStack cli and awslocal 15 docker pull localstack/localstack # Make sure to pull the latest version of the image 16 localstack start -d # Start LocalStack in the background 17 18 echo \u0026#34;Waiting for LocalStack startup...\u0026#34; # Wait 30 seconds for the LocalStack container 19 localstack wait -t 30 # to become ready before timing out 20 echo \u0026#34;Startup complete\u0026#34; 21 - name: Run some Tests against LocalStack 22 run: | 23 awslocal s3 mb s3://test 24 awslocal s3 ls 25 echo \u0026#34;Test Execution complete!\u0026#34; 26 - name: Terraform 27 run: | 28 pip install terraform-local 29 tflocal init 30 ls -la 31 tflocal apply --auto-approve Terratest does appear to show some promising results, it has allowed me to wrap some tests around some Terraform examples I've recently put together.\nAt a basic level it can just a basic init-\u0026gt;plan-apply-\u0026gt;destroy cycle on a Terraform deployment to check it works\n1 2package test 3 4import ( 5\t\u0026#34;testing\u0026#34; 6\t\u0026#34;github.com/gruntwork-io/terratest/modules/terraform\u0026#34; 7) 8 9func TestEksCa(t *testing.T) { 10 11\tterraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ 12\tTerraformDir: \u0026#34;../eks-ca\u0026#34;, 13\t}) 14 15\tdefer terraform.Destroy(t, terraformOptions) 16 17\tterraform.InitAndApply(t, terraformOptions) 18 19} It can also be expanded to query the deployed infrastructure\n1 2package test 3 4import ( 5\t\u0026#34;testing\u0026#34; 6 7\t\u0026#34;github.com/gruntwork-io/terratest/modules/aws\u0026#34; 8\t\u0026#34;github.com/gruntwork-io/terratest/modules/terraform\u0026#34; 9\t\u0026#34;github.com/stretchr/testify/assert\u0026#34; 10) 11 12func TestVpcPeering(t *testing.T) { 13 14\tvpcOwnerPrefix := \u0026#34;10.10\u0026#34; 15\tvpcAcceptorPrefix := \u0026#34;10.20\u0026#34; 16\tawsRegion := \u0026#34;us-east-1\u0026#34; 17\tterraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ 18\tTerraformDir: \u0026#34;../vpc_peering\u0026#34;, 19\tVars: map[string]interface{}{ 20\t\u0026#34;cidr_prefix-a\u0026#34;: vpcOwnerPrefix, 21\t\u0026#34;cidr_prefix-b\u0026#34;: vpcAcceptorPrefix, 22\t}, 23\t}) 24 25\tdefer terraform.Destroy(t, terraformOptions) 26 27\tterraform.InitAndApply(t, terraformOptions) 28 29\tvpcIdOwner := terraform.Output(t, terraformOptions, \u0026#34;vpc-owner-id\u0026#34;) 30\tvpcIdAccepter := terraform.Output(t, terraformOptions, \u0026#34;vpc-accepter-id\u0026#34;) 31 32\tvpcOwner := aws.GetVpcById(t, vpcIdOwner, awsRegion) 33\tvpcAccepter := aws.GetVpcById(t, vpcIdAccepter, awsRegion) 34 35\tassert.NotEmpty(t, vpcOwner.Name) 36\tassert.NotEmpty(t, vpcAccepter.Name) 37} A promising start, it would be nice to pair it with localstack for speed in the future.\n","link":"https://neilarmitage.com/post/terraform_testing/","section":"post","tags":["AWS","terraform"],"title":"Terraform Testing"},{"body":"Transit Gateway allows VPC's to be connected together into a single network as well as connecting to on-prem networks.\nThis example will deploy a simple setup with 2 VPC's being connected together.\nThe Terraform code can be found on GitHub. The code also includes a RAM (Resource Access Manager) share for linking VPC's in separate accounts but in this examples it's not used.\nJust update any the reqruired settings in variables.tf then deploy\n1terraform init 2terraform plan 3terraform apply To remove the example\n1terraform destroy ","link":"https://neilarmitage.com/post/aws_transit_gateway_with_terraform/","section":"post","tags":["AWS","Transit Gateway"],"title":"Setting up AWS Transit Gateway with Terraform"},{"body":"","link":"https://neilarmitage.com/tags/transit-gateway/","section":"tags","tags":null,"title":"Transit Gateway"},{"body":" NATS is a connective technology that powers modern distributed systems. A connective technology is responsible for addressing, discovery and exchanging of messages that drive the common patterns in distributed systems; asking and answering questions, aka services/microservices, and making and processing statements, or stream processing.\nAn example installation using Terraform can be found on GitHub, this deploys a VPC and the supporting infrastructure required along with a AWS instance running Ubuntu with NAT's installed. By default it enables NKEY authentication and stores both the User and Seed Keys in the AWS Parameter Store.\n","link":"https://neilarmitage.com/post/nats_jetstream_in_aws/","section":"post","tags":["AWS"],"title":"Deploying Nats Jetstream in AWS"},{"body":"This example will deploy a VPN into an AWS VPC and show how to connect to it using OpenVPN from either a Mac or Linux host. It will use Certificates for authentication, many other authentication options are available.\nTerraform source code can be found at Github\nIf will deploy a simple VPC in one AZ and then create a VPN, the required certificates for the VPN will be stored in AWS Parameter Store.\nDeployment Check out the code and update the region in variables.tf to the desired region. Make sure you have valid AWS credentials in the environment or update provider.tf with the required values. Run terraform init and terraform apply 1âžœ vpn git:(main) âœ— (âŽˆ |N/A:default) terraform init 2Initializing modules... 3 4Initializing the backend... 5 6Initializing provider plugins... 7- Reusing previous version of hashicorp/aws from the dependency lock file 8- Reusing previous version of hashicorp/tls from the dependency lock file 9- Using previously-installed hashicorp/aws v4.42.0 10- Using previously-installed hashicorp/tls v4.0.4 11 12Terraform has been successfully initialized! 13 14You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see 15any changes that are required for your infrastructure. All Terraform commands 16should now work. 17 18If you ever set or change modules or backend configuration for Terraform, 19rerun this command to reinitialize your working directory. If you forget, other 20commands will detect it and remind you to do so if necessary. 21 22âžœ vpn git:(main) âœ— (âŽˆ |N/A:default) terraform apply 23data.aws_availability_zones.available: Reading... 24data.aws_availability_zones.available: Read complete after 0s [id=us-east-1] 25 26Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: 27 + create 28 29Terraform will perform the following actions: 30 31 # aws_acm_certificate.ca will be created 32 + resource \u0026#34;aws_acm_certificate\u0026#34; \u0026#34;ca\u0026#34; { 33 34.... 35 36aws_ec2_client_vpn_network_association.vpn[0]: Still creating... [8m50s elapsed] 37aws_ec2_client_vpn_network_association.vpn[0]: Still creating... [9m0s elapsed] 38aws_ec2_client_vpn_network_association.vpn[0]: Creation complete after 9m1s [id=cvpn-assoc-0ae03994bfbf5dea9] 39â•· 40â”‚ Warning: Argument is deprecated 41â”‚ 42â”‚ with aws_ec2_client_vpn_network_association.vpn[0], 43â”‚ on vpn.tf line 155, in resource \u0026#34;aws_ec2_client_vpn_network_association\u0026#34; \u0026#34;vpn\u0026#34;: 44â”‚ 155: security_groups = [aws_security_group.vpn.id] 45â”‚ 46â”‚ Use the `security_group_ids` attribute of the `aws_ec2_client_vpn_endpoint` resource instead. 47â•µ 48 49Apply complete! Resources: 30 added, 0 changed, 0 destroyed. 50 51Outputs: 52 53VPN-endpoint = \u0026#34;cvpn-endpoint-0b1df2b162a5679e1\u0026#34; Connecting to the VPN Install OpenVPN brew install openvpn\nGet the parameters\n1aws ssm get-parameter --name \u0026#34;dev_vpn_key\u0026#34; --with-decryption \\ 2 --output text --query Parameter.Value \u0026gt; key.txt 3 4aws ssm get-parameter --name \u0026#34;dev_vpn_certificate\u0026#34; --with-decryption \\ 5 --output text --query Parameter.Value \u0026gt; cert.txt 6 7aws ec2 export-client-vpn-client-configuration \\ 8 --client-vpn-endpoint-id cvpn-endpoint-0b1df2b162a5679e1 \\ 9 --output text \u0026gt; vpn.txt Connect to the VPN\n1âžœ vpn git:(main) âœ— (âŽˆ |N/A:default) sudo openvpn --config vpn.txt \\ 2 --key key.txt --cert cert.txt 3 4Password: 52022-12-02 09:35:06 WARNING: file \u0026#39;key.txt\u0026#39; is group or others accessible 62022-12-02 09:35:06 OpenVPN 2.5.7 arm-apple-darwin21.5.0 [SSL (OpenSSL)] [LZO] [LZ4] [PKCS11] [MH/RECVDA] [AEAD] built on Jun 8 2022 72022-12-02 09:35:06 library versions: OpenSSL 1.1.1s 1 Nov 2022, LZO 2.10 82022-12-02 09:35:06 TCP/UDP: Preserving recently used remote address: [AF_INET]3.234.149.235:443 92022-12-02 09:35:06 Socket Buffers: R=[131072-\u0026gt;131072] S=[131072-\u0026gt;131072] 102022-12-02 09:35:06 Attempting to establish TCP connection with [AF_INET]3.234.149.235:443 [nonblock] 112022-12-02 09:35:06 TCP connection established with [AF_INET]3.234.149.235:443 122022-12-02 09:35:06 TCP_CLIENT link local: (not bound) 132022-12-02 09:35:06 TCP_CLIENT link remote: [AF_INET]3.234.149.235:443 142022-12-02 09:35:06 TLS: Initial packet from [AF_INET]3.234.149.235:443, sid=ee28dcaa 0ddbf149 152022-12-02 09:35:07 VERIFY OK: depth=1, O=demo, CN=dev.vpn.ca 162022-12-02 09:35:07 VERIFY KU OK 172022-12-02 09:35:07 Validating certificate extended key usage 182022-12-02 09:35:07 ++ Certificate has EKU (str) TLS Web Server Authentication, expects TLS Web Server Authentication 192022-12-02 09:35:07 VERIFY EKU OK 202022-12-02 09:35:07 VERIFY OK: depth=0, O=demo, CN=dev.vpn.server 212022-12-02 09:35:07 Control Channel: TLSv1.2, cipher TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, peer certificate: 2048 bit RSA, signature: RSA-SHA256 222022-12-02 09:35:07 [dev.vpn.server] Peer Connection Initiated with [AF_INET]3.234.149.235:443 232022-12-02 09:35:08 SENT CONTROL [dev.vpn.server]: \u0026#39;PUSH_REQUEST\u0026#39; (status=1) 242022-12-02 09:35:08 PUSH: Received control message: \u0026#39;PUSH_REPLY,route 10.0.0.0 255.255.0.0,route-gateway 10.0.240.1,topology subnet,ping 1,ping-restart 20,ifconfig 10.0.240.2 255.255.255.224,peer-id 0,cipher AES-256-GCM\u0026#39; 252022-12-02 09:35:08 OPTIONS IMPORT: timers and/or timeouts modified 262022-12-02 09:35:08 OPTIONS IMPORT: --ifconfig/up options modified 272022-12-02 09:35:08 OPTIONS IMPORT: route options modified 282022-12-02 09:35:08 OPTIONS IMPORT: route-related options modified 292022-12-02 09:35:08 OPTIONS IMPORT: peer-id set 302022-12-02 09:35:08 OPTIONS IMPORT: adjusting link_mtu to 1626 312022-12-02 09:35:08 OPTIONS IMPORT: data channel crypto options modified 322022-12-02 09:35:08 Outgoing Data Channel: Cipher \u0026#39;AES-256-GCM\u0026#39; initialized with 256 bit key 332022-12-02 09:35:08 Incoming Data Channel: Cipher \u0026#39;AES-256-GCM\u0026#39; initialized with 256 bit key 342022-12-02 09:35:08 Opened utun device utun7 352022-12-02 09:35:08 /sbin/ifconfig utun7 delete 36ifconfig: ioctl (SIOCDIFADDR): Can\u0026#39;t assign requested address 372022-12-02 09:35:08 NOTE: Tried to delete pre-existing tun/tap instance -- No Problem if failure 382022-12-02 09:35:08 /sbin/ifconfig utun7 10.0.240.2 10.0.240.2 netmask 255.255.255.224 mtu 1500 up 392022-12-02 09:35:08 /sbin/route add -net 10.0.240.0 10.0.240.2 255.255.255.224 40add net 10.0.240.0: gateway 10.0.240.2 412022-12-02 09:35:08 /sbin/route add -net 10.0.0.0 10.0.240.1 255.255.0.0 42add net 10.0.0.0: gateway 10.0.240.1 432022-12-02 09:35:08 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this 442022-12-02 09:35:08 Initialization Sequence Completed You can now connect directly to the endpoints in the private subnet\nTidy Up 1terraform destroy ","link":"https://neilarmitage.com/post/setting_up_aws_vpn/","section":"post","tags":["AWS","VPN"],"title":"Setting up AWS VPN"},{"body":"","link":"https://neilarmitage.com/tags/vpn/","section":"tags","tags":null,"title":"VPN"},{"body":" I will be Speaking at the Open Source Summit Europe 2022 in Dublin\nThe slides can be found on SpeakerDeck.\n","link":"https://neilarmitage.com/post/cloudopen-europe-2022/","section":"post","tags":["AWS","talks"],"title":"Avoiding pain when operating in the Cloud"},{"body":"","link":"https://neilarmitage.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://neilarmitage.com/categories/cloudopen-talks/","section":"categories","tags":null,"title":"cloudopen talks"},{"body":"","link":"https://neilarmitage.com/tags/talks/","section":"tags","tags":null,"title":"talks"},{"body":"","link":"https://neilarmitage.com/categories/aws-k8s-velero/","section":"categories","tags":null,"title":"AWS K8S velero"},{"body":"","link":"https://neilarmitage.com/tags/eks/","section":"tags","tags":null,"title":"EKS"},{"body":"Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes. It can be setup quickly with Terraform on a EKS cluster and is simple to operate.\nAn example deployment including EKS can be cloned from here\nInstallation via Terraform 1resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;velero\u0026#34; { 2 bucket = \u0026#34;eks-velero-backup-${var.environment_name}\u0026#34; 3 acl = \u0026#34;private\u0026#34; 4 server_side_encryption_configuration { 5 rule { 6 apply_server_side_encryption_by_default { 7 sse_algorithm = \u0026#34;AES256\u0026#34; 8 } 9 } 10 } 11 versioning { 12 enabled = true 13 } 14} 15 16resource \u0026#34;aws_s3_bucket_policy\u0026#34; \u0026#34;velero\u0026#34; { 17 bucket = aws_s3_bucket.velero.id 18 19 policy = jsonencode({ 20 Version = \u0026#34;2012-10-17\u0026#34; 21 Id = \u0026#34;velero-${var.environment_name}-bucket-policy\u0026#34; 22 Statement = [ 23 { 24 Sid = \u0026#34;EnforceTls\u0026#34; 25 Effect = \u0026#34;Deny\u0026#34; 26 Principal = \u0026#34;*\u0026#34; 27 Action = \u0026#34;s3:*\u0026#34; 28 Resource = [ 29 \u0026#34;${aws_s3_bucket.velero.arn}/*\u0026#34;, 30 \u0026#34;${aws_s3_bucket.velero.arn}\u0026#34;, 31 ] 32 Condition = { 33 Bool = { 34 \u0026#34;aws:SecureTransport\u0026#34; = \u0026#34;false\u0026#34; 35 } 36 NumericLessThan = { 37 \u0026#34;s3:TlsVersion\u0026#34;: 1.2 38 } 39 } 40 }, 41 ] 42 }) 43} 44 45module \u0026#34;velero\u0026#34; { 46 source = \u0026#34;DNXLabs/eks-velero/aws\u0026#34; 47 version = \u0026#34;0.1.2\u0026#34; 48 49 enabled = true 50 51 cluster_name = module.eks.cluster_id 52 cluster_identity_oidc_issuer = module.eks.cluster_oidc_issuer_url 53 cluster_identity_oidc_issuer_arn = module.eks.oidc_provider_arn 54 aws_region = var.region 55 create_bucket = false 56 bucket_name = \u0026#34;eks-velero-backup-${var.environment_name}\u0026#34; 57 helm_chart_version = \u0026#34;2.30.1\u0026#34; 58} When a new namespace is created a daily scheduled backup is included in the namespace\n1velero schedule create $NAMESPACE-backup --schedule \u0026#34;0 7 * * *\u0026#34; -n $NAMESPACE 2 3âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero schedule get 4NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR 529681-sys-backup Enabled 2022-07-28 15:31:07 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 6backup-demo-backup Enabled 2022-07-28 08:19:28 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 7dev-backup Enabled 2022-07-28 09:59:44 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 8dev-update-backup Enabled 2022-07-28 09:38:01 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 9develop-backup Enabled 2022-07-28 09:29:47 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 10pfs-retry-backup Enabled 2022-07-28 17:17:55 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 11ui-backup Enabled 2022-07-28 11:14:28 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 12âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) Testing Backup and recovery of a namespace has been tested with a recovery from S3 and EBS snapshots\nA deployment has been created in backup-demo\nA schedule was created 1âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero schedule get 2 3NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR 429681-sys-backup Enabled 2022-07-28 15:31:07 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 5backup-demo-backup Enabled 2022-07-28 08:19:28 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 6dev-backup Enabled 2022-07-28 09:59:44 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 7dev-update-backup Enabled 2022-07-28 09:38:01 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 8develop-backup Enabled 2022-07-28 09:29:47 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 9pfs-retry-backup Enabled 2022-07-28 17:17:55 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; 10ui-backup Enabled 2022-07-28 11:14:28 +0100 BST 0 3 * * * 0s 6h ago \u0026lt;none\u0026gt; A list of backups available 1âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero backup get 2 3NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR 429681-sys-backup-20220729030042 PartiallyFailed 1 0 2022-07-29 04:00:55 +0100 BST 29d default \u0026lt;none\u0026gt; 5backup-demo-backup-20220729030042 PartiallyFailed 6 0 2022-07-29 04:01:30 +0100 BST 29d default \u0026lt;none\u0026gt; 6backup-demo-schedule-20220729030042 Completed 0 0 2022-07-29 04:01:17 +0100 BST 29d default \u0026lt;none\u0026gt; 7backup-demo-schedule-20220728030041 Completed 0 0 2022-07-28 04:00:41 +0100 BST 28d default \u0026lt;none\u0026gt; 8dev-backup-20220729030042 Completed 0 0 2022-07-29 04:02:09 +0100 BST 29d default \u0026lt;none\u0026gt; 9dev-update-backup-20220729030042 Completed 0 0 2022-07-29 04:01:56 +0100 BST 29d default \u0026lt;none\u0026gt; 10develop-backup-20220729030042 Completed 0 0 2022-07-29 04:01:43 +0100 BST 29d default \u0026lt;none\u0026gt; 11pfs-retry-backup-20220729030042 Completed 0 0 2022-07-29 04:01:05 +0100 BST 29d default \u0026lt;none\u0026gt; 12ui-backup-20220729030042 Completed 0 0 2022-07-29 04:00:42 +0100 BST 29d default \u0026lt;none\u0026gt; Test namespace deleted 1âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) k get ns | grep backup-demo 2backup-demo Active 2d18h 3 4âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) k delete ns backup-demo 5namespace \u0026#34;backup-demo\u0026#34; deleted 6 7âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) k get ns | grep backup-demo Backup restored 1âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero restore create --from-backup backup-demo-schedule-20220729030042 2Restore request \u0026#34;backup-demo-schedule-20220729030042-20220729102625\u0026#34; submitted successfully. 3Run `velero restore describe backup-demo-schedule-20220729030042-20220729102625` or `velero restore logs backup-demo-schedule-20220729030042-20220729102625` for more details. 4 5âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero restore describe backup-demo-schedule-20220729030042-20220729102625 6 7Name: backup-demo-schedule-20220729030042-20220729102625 8Namespace: velero 9Labels: \u0026lt;none\u0026gt; 10Annotations: \u0026lt;none\u0026gt; 11 12Phase: InProgress 13Estimated total items to be restored: 201 14Items restored so far: 11 15 16Started: 2022-07-29 10:26:27 +0100 BST 17Completed: \u0026lt;n/a\u0026gt; 18 19Backup: backup-demo-schedule-20220729030042 20 21Namespaces: 22 Included: all namespaces found in the backup 23 Excluded: \u0026lt;none\u0026gt; 24 25Resources: 26 Included: * 27 Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io 28 Cluster-scoped: auto 29 30Namespace mappings: \u0026lt;none\u0026gt; 31 32Label selector: \u0026lt;none\u0026gt; 33 34Restore PVs: auto 35 36Existing Resource Policy: \u0026lt;none\u0026gt; 37 38Preserve Service NodePorts: auto 39âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) Backup restored 1âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) velero restore describe backup-demo-schedule-20220729030042-20220729102625 2 3Name: backup-demo-schedule-20220729030042-20220729102625 4Namespace: velero 5Labels: \u0026lt;none\u0026gt; 6Annotations: \u0026lt;none\u0026gt; 7 8Phase: Completed 9Total items to be restored: 197 10Items restored: 197 11 12Started: 2022-07-29 10:26:27 +0100 BST 13Completed: 2022-07-29 10:27:03 +0100 BST 14 15Warnings: 16 Velero: \u0026lt;none\u0026gt; 17 Cluster: could not restore, CustomResourceDefinition \u0026#34;certificaterequests.cert-manager.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 18 could not restore, CustomResourceDefinition \u0026#34;certificates.cert-manager.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 19 could not restore, CustomResourceDefinition \u0026#34;ciliumendpoints.cilium.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 20 could not restore, CustomResourceDefinition \u0026#34;ciliumnetworkpolicies.cilium.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 21 could not restore, CustomResourceDefinition \u0026#34;orders.acme.cert-manager.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 22 could not restore, CustomResourceDefinition \u0026#34;schedules.velero.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 23 could not restore, CustomResourceDefinition \u0026#34;secretagentconfigurations.secret-agent.secrets.forgerock.io\u0026#34; already exists. Warning: the in-cluster version is different than the backed-up version. 24 Namespaces: \u0026lt;none\u0026gt; 25 26Backup: backup-demo-schedule-20220729030042 27 28Namespaces: 29 Included: all namespaces found in the backup 30 Excluded: \u0026lt;none\u0026gt; 31 32Resources: 33 Included: * 34 Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io 35 Cluster-scoped: auto 36 37Namespace mappings: \u0026lt;none\u0026gt; 38 39Label selector: \u0026lt;none\u0026gt; 40 41Restore PVs: auto 42 43Existing Resource Policy: \u0026lt;none\u0026gt; 44 45Preserve Service NodePorts: auto 46 47âžœ ~ (âŽˆ |arn:aws:eks:eu-west-2:123456789012:cluster/cluster-1:default) k get po -n backup-demo 48 49NAME READY STATUS RESTARTS AGE 50admin-ui-b89f6f748-zf649 2/2 Running 0 58s 51am-dd47498c5-pbbzh 1/2 Running 0 58s 52consent-and-auth-ui-f9486bbc4-vc5q2 3/3 Running 0 58s 53ds-cts-0 2/2 Running 0 58s 54ds-cts-1 2/2 Running 0 58s 55ds-cts-2 2/2 Running 0 57s 56ds-idrepo-0 2/2 Running 0 57s 57ds-idrepo-1 1/2 Running 0 57s 58ds-umarepo-0 2/2 Running 0 57s 59end-user-ui-759f8bb9d7-pnr72 2/2 Running 0 57s 60idm-b478d46cb-vvvcf 1/2 Running 0 57s 61ig-c4fc95547-tdcxh 1/2 Running 0 56s 62login-ui-7c8f994cb6-2rlqm 2/2 Running 0 56s 63rcs-agent-6f6657cdbb-2tmdb 2/2 Running 0 56s ","link":"https://neilarmitage.com/post/k8s-backup-with-velero/","section":"post","tags":["AWS","EKS"],"title":"Kubernetes  Backup with Velero"},{"body":"","link":"https://neilarmitage.com/categories/aws-eks/","section":"categories","tags":null,"title":"AWS EKS"},{"body":"Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters. It is a replacement for the Cluster Autoscaler which has some issues in AWS\nUpdated - 13/12/2022 : The installation is now easier with the new Terraform module - see here for an example\nThis at the moment this example does not work on an acloudguru sandbox account. The supporting files can be found on Github\nDeploy an EKS Cluster\n1export CLUSTER_NAME=\u0026#34;karpenter-demo\u0026#34; 2export AWS_DEFAULT_REGION=\u0026#34;eu-west-1\u0026#34; 3export AWS_ACCOUNT_ID=\u0026#34;$(aws sts get-caller-identity --query Account --output text)\u0026#34; 4 5eksctl create cluster -f - \u0026lt;\u0026lt; EOF 6--- 7apiVersion: eksctl.io/v1alpha5 8kind: ClusterConfig 9metadata: 10 name: ${CLUSTER_NAME} 11 region: ${AWS_DEFAULT_REGION} 12 version: \u0026#34;1.21\u0026#34; 13 tags: 14 karpenter.sh/discovery: ${CLUSTER_NAME} 15managedNodeGroups: 16 - instanceType: t3.small 17 name: ${CLUSTER_NAME}-ng 18 desiredCapacity: 1 19 minSize: 1 20 maxSize: 2 21iam: 22 withOIDC: true 23EOF 24 25aws eks update-kubeconfig --name ${CLUSTER_NAME} --region=${AWS_DEFAULT_REGION} Install and setup Karpenter\n1 2export CLUSTER_ENDPOINT=\u0026#34;$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \u0026#34;cluster.endpoint\u0026#34; --output text)\u0026#34; 3 4TEMPOUT=$(mktemp) 5 6curl -fsSL https://karpenter.sh/v0.6.3/getting-started/cloudformation.yaml \u0026gt; $TEMPOUT \\ 7\u0026amp;\u0026amp; aws cloudformation deploy \\ 8 --stack-name \u0026#34;Karpenter-${CLUSTER_NAME}\u0026#34; \\ 9 --template-file \u0026#34;${TEMPOUT}\u0026#34; \\ 10 --capabilities CAPABILITY_NAMED_IAM \\ 11 --parameter-overrides \u0026#34;ClusterName=${CLUSTER_NAME}\u0026#34; 12 13eksctl create iamidentitymapping \\ 14 --username system:node:{{EC2PrivateDNSName}} \\ 15 --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ 16 --arn \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}\u0026#34; \\ 17 --group system:bootstrappers \\ 18 --group system:nodes 19 20 eksctl create iamserviceaccount \\ 21 --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; --name karpenter --namespace karpenter \\ 22 --role-name \u0026#34;${CLUSTER_NAME}-karpenter\u0026#34; \\ 23 --attach-policy-arn \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}\u0026#34; \\ 24 --role-only \\ 25 --approve 26 27export KARPENTER_IAM_ROLE_ARN=\u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter\u0026#34; 28 29aws iam create-service-linked-role --aws-service-name spot.amazonaws.com 30 31helm repo add karpenter https://charts.karpenter.sh/ 32helm repo update 33 34helm upgrade --install --namespace karpenter --create-namespace \\ 35 karpenter karpenter/karpenter \\ 36 --version v0.6.3 \\ 37 --set serviceAccount.annotations.\u0026#34;eks\\.amazonaws\\.com/role-arn\u0026#34;=${KARPENTER_IAM_ROLE_ARN} \\ 38 --set clusterName=${CLUSTER_NAME} \\ 39 --set clusterEndpoint=${CLUSTER_ENDPOINT} \\ 40 --set aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \\ 41 --wait # for the defaulting webhook to install before creating a Provisioner 42 43cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 44apiVersion: karpenter.sh/v1alpha5 45kind: Provisioner 46metadata: 47 name: default 48spec: 49 requirements: 50 - key: karpenter.sh/capacity-type 51 operator: In 52 values: [\u0026#34;spot\u0026#34;] 53 limits: 54 resources: 55 cpu: 1000 56 provider: 57 subnetSelector: 58 karpenter.sh/discovery: ${CLUSTER_NAME} 59 securityGroupSelector: 60 karpenter.sh/discovery: ${CLUSTER_NAME} 61 ttlSecondsAfterEmpty: 30 62EOF Deploy a test workload and watch the nodes deploy\n1cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 2apiVersion: apps/v1 3kind: Deployment 4metadata: 5 name: inflate 6spec: 7 replicas: 0 8 selector: 9 matchLabels: 10 app: inflate 11 template: 12 metadata: 13 labels: 14 app: inflate 15 spec: 16 terminationGracePeriodSeconds: 0 17 containers: 18 - name: inflate 19 image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 20 resources: 21 requests: 22 cpu: 1 23EOF 24kubectl scale deployment inflate --replicas 5 25kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller 1âžœ eks git:(main) âœ— (âŽˆ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller 22022-05-13T10:26:47.308Z INFO controller.controller.nodemetrics Starting workers {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;reconciler group\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;reconciler kind\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;worker count\u0026#34;: 1} 32022-05-13T10:26:47.308Z INFO controller.controller.provisioning Starting workers {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;reconciler group\u0026#34;: \u0026#34;karpenter.sh\u0026#34;, \u0026#34;reconciler kind\u0026#34;: \u0026#34;Provisioner\u0026#34;, \u0026#34;worker count\u0026#34;: 10} 42022-05-13T10:26:59.993Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 52022-05-13T10:28:22.951Z INFO controller.provisioning Batched 5 pods in 1.176501626s {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 62022-05-13T10:28:23.156Z INFO controller.provisioning Computed packing of 1 node(s) for 5 pod(s) with instance type option(s) [c1.xlarge c4.2xlarge c3.2xlarge c5d.2xlarge c6i.2xlarge c5a.2xlarge c5.2xlarge c6a.2xlarge c5ad.2xlarge c5n.2xlarge m3.2xlarge m5ad.2xlarge t3.2xlarge m5d.2xlarge m5.2xlarge m5zn.2xlarge m5n.2xlarge t3a.2xlarge m5dn.2xlarge m4.2xlarge] {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 72022-05-13T10:28:25.448Z INFO controller.provisioning Launched instance: i-0addfe38342f0960a, hostname: ip-192-168-111-106.eu-west-1.compute.internal, type: t3.2xlarge, zone: eu-west-1c, capacityType: spot {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 82022-05-13T10:28:25.515Z INFO controller.provisioning Bound 5 pod(s) to node ip-192-168-111-106.eu-west-1.compute.internal {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 92022-05-13T10:28:25.515Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 102022-05-13T10:28:26.517Z INFO controller.provisioning Batched 1 pods in 1.00078258s {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 112022-05-13T10:28:26.519Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} Scale the deployment down and watch the nodes terminate\n1kubectl delete deployment inflate 2kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller 1deployment.apps \u0026#34;inflate\u0026#34; deleted 22022-05-13T10:26:47.308Z INFO controller.controller.provisioning Starting workers {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;reconciler group\u0026#34;: \u0026#34;karpenter.sh\u0026#34;, \u0026#34;reconciler kind\u0026#34;: \u0026#34;Provisioner\u0026#34;, \u0026#34;worker count\u0026#34;: 10} 32022-05-13T10:26:59.993Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 42022-05-13T10:28:22.951Z INFO controller.provisioning Batched 5 pods in 1.176501626s {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 52022-05-13T10:28:23.156Z INFO controller.provisioning Computed packing of 1 node(s) for 5 pod(s) with instance type option(s) [c1.xlarge c4.2xlarge c3.2xlarge c5d.2xlarge c6i.2xlarge c5a.2xlarge c5.2xlarge c6a.2xlarge c5ad.2xlarge c5n.2xlarge m3.2xlarge m5ad.2xlarge t3.2xlarge m5d.2xlarge m5.2xlarge m5zn.2xlarge m5n.2xlarge t3a.2xlarge m5dn.2xlarge m4.2xlarge] {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 62022-05-13T10:28:25.448Z INFO controller.provisioning Launched instance: i-0addfe38342f0960a, hostname: ip-192-168-111-106.eu-west-1.compute.internal, type: t3.2xlarge, zone: eu-west-1c, capacityType: spot {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 72022-05-13T10:28:25.515Z INFO controller.provisioning Bound 5 pod(s) to node ip-192-168-111-106.eu-west-1.compute.internal {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 82022-05-13T10:28:25.515Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 92022-05-13T10:28:26.517Z INFO controller.provisioning Batched 1 pods in 1.00078258s {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 102022-05-13T10:28:26.519Z INFO controller.provisioning Waiting for unschedulable pods {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;provisioner\u0026#34;: \u0026#34;default\u0026#34;} 112022-05-13T10:31:51.183Z INFO controller.node Added TTL to empty node {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ip-192-168-111-106.eu-west-1.compute.internal\u0026#34;} 122022-05-13T10:32:21.001Z INFO controller.node Triggering termination after 30s for empty node {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ip-192-168-111-106.eu-west-1.compute.internal\u0026#34;} 132022-05-13T10:32:21.031Z INFO controller.termination Cordoned node {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ip-192-168-111-106.eu-west-1.compute.internal\u0026#34;} 142022-05-13T10:32:21.239Z INFO controller.termination Deleted node {\u0026#34;commit\u0026#34;: \u0026#34;fd19ba2\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ip-192-168-111-106.eu-west-1.compute.internal\u0026#34;} Tidy up the cluster\n1helm uninstall karpenter --namespace karpenter 2aws iam delete-role --role-name \u0026#34;${CLUSTER_NAME}-karpenter\u0026#34; 3aws cloudformation delete-stack --stack-name \u0026#34;Karpenter-${CLUSTER_NAME}\u0026#34; 4aws ec2 describe-launch-templates \\ 5 | jq -r \u0026#34;.LaunchTemplates[].LaunchTemplateName\u0026#34; \\ 6 | grep -i \u0026#34;Karpenter-${CLUSTER_NAME}\u0026#34; \\ 7 | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} 8eksctl delete cluster --name \u0026#34;${CLUSTER_NAME}\u0026#34; ","link":"https://neilarmitage.com/post/karpenter/","section":"post","tags":["AWS","EKS"],"title":"Installing Karpenter"},{"body":"EKSCTL can be used to quickly deploy a AWS EKS Cluster.\nThis is based on using a sandbox AWS account The supporting files can be found on Github\nCreate an EKS deployment file, I tend to create individual nodegroups dedicated to a single AZ\n1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ cat ca-cluster.yaml 3 4--- 5apiVersion: eksctl.io/v1alpha5 6kind: ClusterConfig 7 8metadata: 9 name: ca-cluster 10--- 11apiVersion: eksctl.io/v1alpha5 12kind: ClusterConfig 13 14metadata: 15 name: ca-cluster 16 region: us-east-1 17 version: \u0026#34;1.22\u0026#34; 18 19availabilityZones: [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;, \u0026#34;us-east-1c\u0026#34;] 20 21managedNodeGroups: 22- name: nodegroupA 23 desiredCapacity: 1 24 availabilityZones: [\u0026#34;us-east-1a\u0026#34;] 25 instanceType: t3.small 26 spot: true 27- name: nodegroupB 28 desiredCapacity: 1 29 availabilityZones: [\u0026#34;us-east-1b\u0026#34;] 30 instanceType: t3.small 31 spot: true 32- name: nodegroupC 33 desiredCapacity: 1 34 availabilityZones: [\u0026#34;us-east-1c\u0026#34;] 35 instanceType: t3.small 36 spot: true Create the cluster (will take a long time)\n1[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ eksctl create cluster -f ca-cluster.yaml 22022-05-12 08:03:06 [â„¹] eksctl version 0.96.0 32022-05-12 08:03:06 [â„¹] using region us-east-1 42022-05-12 08:03:06 [â„¹] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19 52022-05-12 08:03:06 [â„¹] subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19 62022-05-12 08:03:06 [â„¹] subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19 72022-05-12 08:03:06 [â„¹] nodegroup \u0026#34;nodegroupA\u0026#34; will use \u0026#34;\u0026#34; [AmazonLinux2/1.22] 82022-05-12 08:03:06 [â„¹] nodegroup \u0026#34;nodegroupB\u0026#34; will use \u0026#34;\u0026#34; [AmazonLinux2/1.22] 92022-05-12 08:03:06 [â„¹] nodegroup \u0026#34;nodegroupC\u0026#34; will use \u0026#34;\u0026#34; [AmazonLinux2/1.22] 102022-05-12 08:03:06 [â„¹] using Kubernetes version 1.22 112022-05-12 08:03:06 [â„¹] creating EKS cluster \u0026#34;ca-cluster\u0026#34; in \u0026#34;us-east-1\u0026#34; region with managed nodes 122022-05-12 08:03:06 [â„¹] 3 nodegroups (nodegroupA, nodegroupB, nodegroupC) were included (based on the include/exclude rules) 132022-05-12 08:03:06 [â„¹] will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s) 142022-05-12 08:03:06 [â„¹] will create a CloudFormation stack for cluster itself and 3 managed nodegroup stack(s) 152022-05-12 08:03:06 [â„¹] if you encounter any issues, check CloudFormation console or try \u0026#39;eksctl utils describe-stacks --region=us-east-1 --cluster=ca-cluster\u0026#39; 162022-05-12 08:03:06 [â„¹] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \u0026#34;ca-cluster\u0026#34; in \u0026#34;us-east-1\u0026#34; 172022-05-12 08:03:06 [â„¹] CloudWatch logging will not be enabled for cluster \u0026#34;ca-cluster\u0026#34; in \u0026#34;us-east-1\u0026#34; 182022-05-12 08:03:06 [â„¹] you can enable it with \u0026#39;eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=ca-cluster\u0026#39; 192022-05-12 08:03:06 [â„¹] 202 sequential tasks: { create cluster control plane \u0026#34;ca-cluster\u0026#34;, 21 2 sequential sub-tasks: { 22 wait for control plane to become ready, 23 3 parallel sub-tasks: { 24 create managed nodegroup \u0026#34;nodegroupA\u0026#34;, 25 create managed nodegroup \u0026#34;nodegroupB\u0026#34;, 26 create managed nodegroup \u0026#34;nodegroupC\u0026#34;, 27 }, 28 } 29} 302022-05-12 08:03:06 [â„¹] building cluster stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 312022-05-12 08:03:06 [â„¹] deploying stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 322022-05-12 08:03:36 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 332022-05-12 08:04:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 342022-05-12 08:05:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 352022-05-12 08:06:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 362022-05-12 08:07:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 372022-05-12 08:08:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 382022-05-12 08:09:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 392022-05-12 08:10:06 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 402022-05-12 08:11:07 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 412022-05-12 08:12:07 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 422022-05-12 08:13:07 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 432022-05-12 08:14:07 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-cluster\u0026#34; 442022-05-12 08:16:08 [â„¹] building managed nodegroup stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 452022-05-12 08:16:08 [â„¹] building managed nodegroup stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 462022-05-12 08:16:08 [â„¹] building managed nodegroup stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 472022-05-12 08:16:08 [â„¹] deploying stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 482022-05-12 08:16:08 [â„¹] deploying stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 492022-05-12 08:16:08 [â„¹] deploying stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 502022-05-12 08:16:08 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 512022-05-12 08:16:08 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 522022-05-12 08:16:08 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 532022-05-12 08:16:38 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 542022-05-12 08:16:38 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 552022-05-12 08:16:38 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 562022-05-12 08:17:19 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 572022-05-12 08:17:19 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 582022-05-12 08:17:25 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 592022-05-12 08:18:07 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 602022-05-12 08:18:40 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 612022-05-12 08:18:47 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 622022-05-12 08:19:08 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 632022-05-12 08:20:26 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 642022-05-12 08:20:34 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 652022-05-12 08:20:35 [â„¹] waiting for the control plane availability... 662022-05-12 08:20:35 [âœ”] saved kubeconfig as \u0026#34;/home/cloudshell-user/.kube/config\u0026#34; 672022-05-12 08:20:35 [â„¹] no tasks 682022-05-12 08:20:35 [âœ”] all EKS cluster resources for \u0026#34;ca-cluster\u0026#34; have been created 692022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupA\u0026#34; has 1 node(s) 702022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-11-66.ec2.internal\u0026#34; is ready 712022-05-12 08:20:35 [â„¹] waiting for at least 1 node(s) to become ready in \u0026#34;nodegroupA\u0026#34; 722022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupA\u0026#34; has 1 node(s) 732022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-11-66.ec2.internal\u0026#34; is ready 742022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupB\u0026#34; has 1 node(s) 752022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-51-71.ec2.internal\u0026#34; is ready 762022-05-12 08:20:35 [â„¹] waiting for at least 1 node(s) to become ready in \u0026#34;nodegroupB\u0026#34; 772022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupB\u0026#34; has 1 node(s) 782022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-51-71.ec2.internal\u0026#34; is ready 792022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupC\u0026#34; has 1 node(s) 802022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-73-100.ec2.internal\u0026#34; is ready 812022-05-12 08:20:35 [â„¹] waiting for at least 1 node(s) to become ready in \u0026#34;nodegroupC\u0026#34; 822022-05-12 08:20:35 [â„¹] nodegroup \u0026#34;nodegroupC\u0026#34; has 1 node(s) 832022-05-12 08:20:35 [â„¹] node \u0026#34;ip-192-168-73-100.ec2.internal\u0026#34; is ready 842022-05-12 08:20:35 [âœ–] parsing kubectl version string (upstream error: error: exec plugin: invalid apiVersion \u0026#34;client.authentication.k8s.io/v1alpha1\u0026#34; 85) / \u0026#34;0.0.0\u0026#34;: Version string empty 862022-05-12 08:20:35 [â„¹] cluster should be functional despite missing (or misconfigured) client binaries 872022-05-12 08:20:35 [âœ”] EKS cluster \u0026#34;ca-cluster\u0026#34; in \u0026#34;us-east-1\u0026#34; region is ready Download the kubeconfig\n1[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ aws eks update-kubeconfig --name ca-cluster --region=us-east-1 2Added new context arn:aws:eks:us-east-1:566428305604:cluster/ca-cluster to /home/cloudshell-user/.kube/config Check the cluster is running ok\n1[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3ip-192-168-11-66.ec2.internal Ready \u0026lt;none\u0026gt; 17m v1.22.6-eks-7d68063 4ip-192-168-51-71.ec2.internal Ready \u0026lt;none\u0026gt; 17m v1.22.6-eks-7d68063 5ip-192-168-73-100.ec2.internal Ready \u0026lt;none\u0026gt; 17m v1.22.6-eks-7d68063 Update the Auto scalling groups to add extra nodes and a limit\n1[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;$CLUSTER_NAME\u0026#39;]].AutoScalingGroupName\u0026#34; --output text \u0026gt; asg.txt 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ for x in $(cat asg.txt); do aws autoscaling update-auto-scaling-group --auto-scaling-group-name $x --min-size 1 --desired-capacity 1 --max-size 2; done Create the IAM Policy and ServiceAccount for the cluster autoscaler\n1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ aws iam create-policy --policy-name ${CLUSTER_NAME}-k8s-asg-policy --policy-document file://k8s-asg-policy.json 3{ 4 \u0026#34;Policy\u0026#34;: { 5 \u0026#34;PolicyName\u0026#34;: \u0026#34;ca-cluster-k8s-asg-policy\u0026#34;, 6 \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPAYHYOCBDCEETZ2NA33\u0026#34;, 7 \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::566428305604:policy/ca-cluster-k8s-asg-policy\u0026#34;, 8 \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, 9 \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, 10 \u0026#34;AttachmentCount\u0026#34;: 0, 11 \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, 12 \u0026#34;IsAttachable\u0026#34;: true, 13 \u0026#34;CreateDate\u0026#34;: \u0026#34;2022-05-12T08:26:23+00:00\u0026#34;, 14 \u0026#34;UpdateDate\u0026#34;: \u0026#34;2022-05-12T08:26:23+00:00\u0026#34; 15 } 16} 17 18[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ eksctl create iamserviceaccount --name cluster-autoscaler --namespace kube-system --cluster $CLUSTER_NAME --attach-policy-arn \u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/${CLUSTER_NAME}-k8s-asg-policy\u0026#34; --approve --override-existing-serviceaccounts Deploy the CA\n1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ envsubst \u0026lt; cluster-autoscaler-autodiscover.yaml | kubectl apply -f - 3clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created 4role.rbac.authorization.k8s.io/cluster-autoscaler created 5clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created 6rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created 7deployment.apps/cluster-autoscaler created 8 9[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026#34;false\u0026#34; 10deployment.apps/cluster-autoscaler annotated 11 12[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ export AUTOSCALER_VERSION=1.22.2 13[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ kubectl -n kube-system \\ 14\u0026gt; set image deployment.apps/cluster-autoscaler \\ 15\u0026gt; cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} 16deployment.apps/cluster-autoscaler image updated Delete the cluster\n1 2[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ eksctl delete cluster ca-cluster 32022-05-12 08:36:38 [â„¹] eksctl version 0.96.0 42022-05-12 08:36:38 [â„¹] using region us-east-1 52022-05-12 08:36:38 [â„¹] deleting EKS cluster \u0026#34;ca-cluster\u0026#34; 62022-05-12 08:36:39 [â„¹] will drain 0 unmanaged nodegroup(s) in cluster \u0026#34;ca-cluster\u0026#34; 72022-05-12 08:36:39 [â„¹] deleted 0 Fargate profile(s) 82022-05-12 08:36:39 [âœ”] kubeconfig has been updated 92022-05-12 08:36:39 [â„¹] cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress 102022-05-12 08:36:48 [â„¹] 113 sequential tasks: { 12 3 parallel sub-tasks: { 13 delete nodegroup \u0026#34;nodegroupC\u0026#34;, 14 delete nodegroup \u0026#34;nodegroupA\u0026#34;, 15 delete nodegroup \u0026#34;nodegroupB\u0026#34;, 16 }, 17 2 sequential sub-tasks: { 18 2 sequential sub-tasks: { 19 delete IAM role for serviceaccount \u0026#34;kube-system/cluster-autoscaler\u0026#34;, 20 delete serviceaccount \u0026#34;kube-system/cluster-autoscaler\u0026#34;, 21 }, 22 delete IAM OIDC provider, 23 }, delete cluster control plane \u0026#34;ca-cluster\u0026#34; [async] 24} 252022-05-12 08:36:48 [â„¹] will delete stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 262022-05-12 08:36:48 [â„¹] waiting for stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; to get deleted 272022-05-12 08:36:48 [â„¹] will delete stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 282022-05-12 08:36:48 [â„¹] waiting for stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; to get deleted 292022-05-12 08:36:48 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 302022-05-12 08:36:49 [â„¹] will delete stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 312022-05-12 08:36:49 [â„¹] waiting for stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; to get deleted 322022-05-12 08:36:49 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 332022-05-12 08:36:49 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 342022-05-12 08:37:18 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 352022-05-12 08:37:19 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 362022-05-12 08:37:19 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 372022-05-12 08:37:51 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 382022-05-12 08:37:54 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 392022-05-12 08:38:09 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 402022-05-12 08:38:29 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 412022-05-12 08:39:49 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 422022-05-12 08:39:54 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 432022-05-12 08:40:14 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 442022-05-12 08:40:32 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 452022-05-12 08:40:51 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 462022-05-12 08:41:12 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 472022-05-12 08:41:35 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 482022-05-12 08:42:13 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 492022-05-12 08:42:46 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 502022-05-12 08:42:57 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 512022-05-12 08:43:05 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 522022-05-12 08:43:35 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 532022-05-12 08:44:23 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 542022-05-12 08:44:47 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupC\u0026#34; 552022-05-12 08:45:04 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupB\u0026#34; 562022-05-12 08:46:00 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-nodegroup-nodegroupA\u0026#34; 572022-05-12 08:46:02 [â„¹] will delete stack \u0026#34;eksctl-ca-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler\u0026#34; 582022-05-12 08:46:02 [â„¹] waiting for stack \u0026#34;eksctl-ca-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler\u0026#34; to get deleted 592022-05-12 08:46:04 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler\u0026#34; 602022-05-12 08:46:34 [â„¹] waiting for CloudFormation stack \u0026#34;eksctl-ca-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler\u0026#34; 612022-05-12 08:46:34 [â„¹] deleted serviceaccount \u0026#34;kube-system/cluster-autoscaler\u0026#34; 622022-05-12 08:46:34 [â„¹] 1 error(s) occurred while deleting cluster with nodegroup(s) 632022-05-12 08:46:34 [âœ–] deleting OIDC provider: operation error IAM: DeleteOpenIDConnectProvider, https response error StatusCode: 403, RequestID: 3944137c-c5ff-4263-8156-db113d89d620, api error AccessDenied: User: arn:aws:iam::566428305604:user/cloud_user is not authorized to perform: iam:DeleteOpenIDConnectProvider on resource: arn:aws:iam::566428305604:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/7B0C3F25F3AC9E9E3C2EEDB41AC61FB9 with an explicit deny in a service control policy 64Error: failed to delete cluster with nodegroup(s) 65[cloudshell-user@ip-10-1-181-252 cluster-autoscaler]$ ","link":"https://neilarmitage.com/post/basic-eks-cluster/","section":"post","tags":["AWS","EKS"],"title":"Basic EKS Cluster with Cluster Autoscaler"},{"body":"Cilium can run in chaining mode which allows it to run alongside the AWS-CNI plugin.\nThis is based on using a sandbox AWS account The supporting files can be found on Github\nInstall some extra tools\n1curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} 2sha256sum --check cilium-linux-amd64.tar.gz.sha256sum 3sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin 4rm cilium-linux-amd64.tar.gz{,.sha256sum} 5 6 7export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt) 8curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum} 9sha256sum --check hubble-linux-amd64.tar.gz.sha256sum 10sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin 11rm hubble-linux-amd64.tar.gz{,.sha256sum} Deploy an EKS Cluster\n1export AWS_REGION=us-east-1 2export AWS_DEFAULT_REGION=us-east-1 3export CLUSTER_NAME=cilium-cluster2 4export ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) 5export CILIUM_NAMESPACE=kube-system 6 7eksctl create cluster -f - \u0026lt;\u0026lt; EOF 8--- 9apiVersion: eksctl.io/v1alpha5 10kind: ClusterConfig 11 12metadata: 13 name: ${CLUSTER_NAME} 14 region: ${AWS_DEFAULT_REGION} 15 version: \u0026#34;1.22\u0026#34; 16 17availabilityZones: [\u0026#34;${AWS_DEFAULT_REGION}a\u0026#34;,\u0026#34;${AWS_DEFAULT_REGION}b\u0026#34;] 18managedNodeGroups: 19 - instanceType: t3.medium 20 name: ${CLUSTER_NAME}-ng 21 desiredCapacity: 2 22 minSize: 1 23 maxSize: 2 24 25EOF 26 27aws eks update-kubeconfig --name ${CLUSTER_NAME} --region=${AWS_DEFAULT_REGION} Install Cilium in chaining mode\n1 2helm repo add cilium https://helm.cilium.io/ 3 4helm install cilium cilium/cilium --version 1.11.3 \\ 5 --namespace kube-system \\ 6 --set cni.chainingMode=aws-cni \\ 7 --set enableIPv4Masquerade=false \\ 8 --set tunnel=disabled \\ 9 --set hubble.listenAddress=\u0026#34;:4244\u0026#34; \\ 10 --set hubble.relay.enabled=true \\ 11 --set hubble.ui.enabled=true 12 13 [cloudshell-user@ip-10-0-84-41 cilium]$ helm install cilium cilium/cilium --version 1.11.3 \\ 14\u0026gt; --namespace kube-system \\ 15\u0026gt; --set cni.chainingMode=aws-cni \\ 16\u0026gt; --set enableIPv4Masquerade=false \\ 17\u0026gt; --set tunnel=disabled \\ 18\u0026gt; --set hubble.listenAddress=\u0026#34;:4244\u0026#34; \\ 19\u0026gt; --set hubble.relay.enabled=true \\ 20\u0026gt; --set hubble.ui.enabled=true 21W0512 14:20:59.634900 8693 warnings.go:70] spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[1].matchExpressions[0].key: beta.kubernetes.io/os is deprecated since v1.14; use \u0026#34;kubernetes.io/os\u0026#34; instead 22W0512 14:20:59.634940 8693 warnings.go:70] spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the \u0026#34;priorityClassName\u0026#34; field instead 23NAME: cilium 24LAST DEPLOYED: Thu May 12 14:20:58 2022 25NAMESPACE: kube-system 26STATUS: deployed 27REVISION: 1 28TEST SUITE: None 29NOTES: 30You have successfully installed Cilium with Hubble Relay and Hubble UI. 31 32Your release version is 1.11.3. 33 34For any further help, visit https://docs.cilium.io/en/v1.11/gettinghelp 35[cloudshell-user@ip-10-0-84-41 cilium]$ kubectl get pod -A 36NAMESPACE NAME READY STATUS RESTARTS AGE 37kube-system aws-node-5lwk4 1/1 Running 0 9m43s 38kube-system aws-node-8b8zs 1/1 Running 0 9m36s 39kube-system cilium-hm8mf 0/1 Init:0/2 0 5s 40kube-system cilium-operator-66bff48676-6kzpb 0/1 ContainerCreating 0 5s 41kube-system cilium-operator-66bff48676-xdmlx 0/1 ContainerCreating 0 5s 42kube-system cilium-pcjmw 0/1 Init:0/2 0 4s 43kube-system coredns-7f5998f4c-crxct 1/1 Running 0 19m 44kube-system coredns-7f5998f4c-zf6df 1/1 Running 0 19m 45kube-system hubble-relay-8548d8946c-x9jpj 0/1 ContainerCreating 0 5s 46kube-system hubble-ui-5f7cdc86c7-nfbkm 0/3 ContainerCreating 0 5s 47kube-system kube-proxy-89sjz 1/1 Running 0 9m43s 48kube-system kube-proxy-smp5s 1/1 Running 0 9m36s 49[cloudshell-user@ip-10-0-84-41 cilium]$ kubectl get pod -A 50NAMESPACE NAME READY STATUS RESTARTS AGE 51kube-system aws-node-5lwk4 1/1 Running 0 9m54s 52kube-system aws-node-8b8zs 1/1 Running 0 9m47s 53kube-system cilium-hm8mf 1/1 Running 0 16s 54kube-system cilium-operator-66bff48676-6kzpb 1/1 Running 0 16s 55kube-system cilium-operator-66bff48676-xdmlx 1/1 Running 0 16s 56kube-system cilium-pcjmw 0/1 Init:1/2 0 15s 57kube-system coredns-7f5998f4c-jg5zx 1/1 Running 0 7s 58kube-system coredns-7f5998f4c-zf6df 1/1 Running 0 19m 59kube-system hubble-relay-8548d8946c-x9jpj 1/1 Running 0 16s 60kube-system hubble-ui-5f7cdc86c7-nfbkm 3/3 Running 0 16s 61kube-system kube-proxy-89sjz 1/1 Running 0 9m54s 62kube-system kube-proxy-smp5s 1/1 Running 0 9m47s 63[cloudshell-user@ip-10-0-84-41 cilium]$ kubectl get pod -A 64NAMESPACE NAME READY STATUS RESTARTS AGE 65kube-system aws-node-5lwk4 1/1 Running 0 9m58s 66kube-system aws-node-8b8zs 1/1 Running 0 9m51s 67kube-system cilium-hm8mf 1/1 Running 0 20s 68kube-system cilium-operator-66bff48676-6kzpb 1/1 Running 0 20s 69kube-system cilium-operator-66bff48676-xdmlx 1/1 Running 0 20s 70kube-system cilium-pcjmw 0/1 Running 0 19s 71kube-system coredns-7f5998f4c-jg5zx 1/1 Running 0 11s 72kube-system coredns-7f5998f4c-zf6df 1/1 Running 0 19m 73kube-system hubble-relay-8548d8946c-x9jpj 1/1 Running 0 20s 74kube-system hubble-ui-5f7cdc86c7-nfbkm 3/3 Running 0 20s 75kube-system kube-proxy-89sjz 1/1 Running 0 9m58s 76kube-system kube-proxy-smp5s 1/1 Running 0 9m51s Install the cluster autoscaler\n1 2 eksctl utils associate-iam-oidc-provider \\ 3 --cluster $CLUSTER_NAME \\ 4 --approve 5 6aws iam create-policy \\ 7 --policy-name ${CLUSTER_NAME}-k8s-asg-policy \\ 8 --policy-document file://k8s-asg-policy.json 9 10eksctl create iamserviceaccount \\ 11 --name cluster-autoscaler \\ 12 --namespace kube-system \\ 13 --cluster $CLUSTER_NAME \\ 14 --attach-policy-arn \u0026#34;arn:aws:iam::${ACCOUNT_ID}:policy/${CLUSTER_NAME}-k8s-asg-policy\u0026#34; \\ 15 --approve \\ 16 --override-existing-serviceaccounts 17 18envsubst \u0026lt; cluster-autoscaler-autodiscover.yaml | kubectl apply -f - 19 20kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026#34;false\u0026#34; 21 22export AUTOSCALER_VERSION=1.22.2 23kubectl -n kube-system \\ 24 set image deployment.apps/cluster-autoscaler \\ 25 cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} Check everything is up and running\n1kubectl port-forward -n $CILIUM_NAMESPACE svc/hubble-relay --address 0.0.0.0 --address :: 4245:80 \u0026amp; 2 3hubble --server localhost:4245 status 4hubble --server localhost:4245 observe 5 6 7[cloudshell-user@ip-10-0-84-41 cilium]$ hubble --server localhost:4245 status 8Handling connection for 4245 9Healthcheck (via localhost:4245): Ok 10Current/Max Flows: 1,336/8,190 (16.31%) 11Flows/s: 4.46 12Connected Nodes: 2/2 13[cloudshell-user@ip-10-0-84-41 cilium]$ hubble --server localhost:4245 observe 14Handling connection for 4245 15May 12 14:25:43.500: 192.168.37.166:39544 \u0026lt;- kube-system/coredns-7f5998f4c-6tln5:8080 to-stack FORWARDED (TCP Flags: ACK, FIN) 16May 12 14:25:43.500: 192.168.37.166:39542 -\u0026gt; kube-system/coredns-7f5998f4c-6tln5:8080 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 17May 12 14:25:43.500: 192.168.37.166:39544 -\u0026gt; kube-system/coredns-7f5998f4c-6tln5:8080 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 18May 12 14:25:44.236: 192.168.37.166:42346 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: SYN) 19May 12 14:25:44.236: 192.168.37.166:42346 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: SYN, ACK) 20May 12 14:25:44.236: 192.168.37.166:42346 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK) 21May 12 14:25:44.236: 192.168.37.166:42348 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: SYN) 22May 12 14:25:44.236: 192.168.37.166:42348 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: SYN, ACK) 23May 12 14:25:44.236: 192.168.37.166:42348 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK) 24May 12 14:25:44.237: 192.168.37.166:42348 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 25May 12 14:25:44.238: 192.168.37.166:42346 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 26May 12 14:25:44.239: 192.168.37.166:42348 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: ACK, PSH) 27May 12 14:25:44.239: 192.168.37.166:42348 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: ACK, FIN) 28May 12 14:25:44.239: 192.168.37.166:42346 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: ACK, PSH) 29May 12 14:25:44.239: 192.168.37.166:42346 \u0026lt;- ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-stack FORWARDED (TCP Flags: ACK, FIN) 30May 12 14:25:44.239: 192.168.37.166:42348 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 31May 12 14:25:44.239: 192.168.37.166:42346 -\u0026gt; ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:10254 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 32May 12 14:25:45.375: ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:40654 \u0026lt;- 192.168.108.176:443 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 33May 12 14:25:45.376: ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:40654 -\u0026gt; 192.168.108.176:443 to-stack FORWARDED (TCP Flags: ACK) 34May 12 14:25:48.079: kube-system/cluster-autoscaler-85d889ffd8-q789m:55780 \u0026lt;- 54.239.31.45:443 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 35May 12 14:25:48.079: kube-system/cluster-autoscaler-85d889ffd8-q789m:55780 -\u0026gt; 54.239.31.45:443 to-stack FORWARDED (TCP Flags: ACK, PSH) 36May 12 14:25:48.079: kube-system/cluster-autoscaler-85d889ffd8-q789m:55780 -\u0026gt; 54.239.31.45:443 to-stack FORWARDED (TCP Flags: ACK, FIN) 37May 12 14:25:48.080: kube-system/cluster-autoscaler-85d889ffd8-q789m:55780 \u0026lt;- 54.239.31.45:443 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 38May 12 14:25:48.080: kube-system/cluster-autoscaler-85d889ffd8-q789m:55780 -\u0026gt; 54.239.31.45:443 to-stack FORWARDED (TCP Flags: ACK) 39May 12 14:25:48.168: kube-system/cluster-autoscaler-85d889ffd8-q789m:52022 -\u0026gt; 192.168.94.37:443 to-stack FORWARDED (TCP Flags: ACK, PSH) 40May 12 14:25:48.179: kube-system/cluster-autoscaler-85d889ffd8-q789m:52022 \u0026lt;- 192.168.94.37:443 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 41May 12 14:25:48.514: 192.168.14.247:56902 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: SYN) 42May 12 14:25:48.514: 192.168.14.247:56902 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: SYN, ACK) 43May 12 14:25:48.514: 192.168.14.247:56902 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: ACK) 44May 12 14:25:48.514: 192.168.14.247:56902 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 45May 12 14:25:48.514: 192.168.14.247:56904 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: SYN) 46May 12 14:25:48.514: 192.168.14.247:56904 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: SYN, ACK) 47May 12 14:25:48.514: 192.168.14.247:56904 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: ACK) 48May 12 14:25:48.514: 192.168.14.247:56902 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: ACK, PSH) 49May 12 14:25:48.514: 192.168.14.247:56902 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: ACK, FIN) 50May 12 14:25:48.514: 192.168.14.247:56902 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: ACK, FIN) 51May 12 14:25:48.514: 192.168.14.247:56904 -\u0026gt; kube-system/coredns-7f5998f4c-r84mj:8080 to-endpoint FORWARDED (TCP Flags: ACK, PSH) 52May 12 14:25:48.514: 192.168.14.247:56904 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: ACK, PSH) 53May 12 14:25:48.514: 192.168.14.247:56904 \u0026lt;- kube-system/coredns-7f5998f4c-r84mj:8080 to-stack FORWARDED (TCP Flags: ACK, FIN) 54May 12 14:25:52.119: ingress-nginx/ingress-nginx-controller-54d8b558d4-gmz56:40654 \u0026lt;- 192.168.108.176:443 to-endpoint FORWARDED (TCP Flags: ACK, PSH) ","link":"https://neilarmitage.com/post/cilium-chaining/","section":"post","tags":["AWS","EKS"],"title":"EKS with Cilium in chaining mode"},{"body":"","link":"https://neilarmitage.com/categories/aws/","section":"categories","tags":null,"title":"AWS"},{"body":"Running your own AWS account for testing can lead to unexpected costs. Unless care is taken around securing the account, the account can be hijacked and used for other purposes. Personally Iâ€™ve stopped running my own accounts and moved to using the sandbox accounts provided by acloudguru as part of their Personal Plus subscription.\nMost of the examples provided in the blog should run on these playgrounds with some exceptions. Generally the setup is using the AWS CloudShell within the sandbox account to remove any issue with local machine setups.\nTo save repeating the setup on CloudShell - the following commands need to be run to setup tools needed for the examples\n1curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; 2sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl 3sudo yum -y install jq gettext bash-completion moreutils envsubst openssl 4curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp 5sudo mv -v /tmp/eksctl /usr/local/bin 6eksctl completion bash \u0026gt;\u0026gt; ~/.bash_completion 7. /etc/profile.d/bash_completion.sh 8. ~/.bash_completion 9 10curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash Download Link : cloudshell.sh\n","link":"https://neilarmitage.com/post/acloudguru-sandbox/","section":"post","tags":null,"title":"Using ACloudGuru AWS Sandboxes"},{"body":"","link":"https://neilarmitage.com/archives/","section":"","tags":null,"title":""},{"body":"My main aim for this site is to write simple HOWTO blogs about cloud based things. I've become increasing annoyed with either video blogs or over complex blogs where the author is trying to show how clever they are.\nI don't claim to be an expert in anything, I've been around long enough to know anyone claiming to be an expert in any cloud technology is probably lying to themselves.\nI have been kicking around in IT for over 35 years now in various companies and startups including Skyscanner, VMWare, Continuent and at the moment I'm a Consultant at Amido (now Ensono Digital).\nAll views are my own and unrelated to any employer past, present or future.\nSome of my slidedecks can now be found on speakdeck\nSome of the images have been created using excalidraw\n","link":"https://neilarmitage.com/about/","section":"","tags":null,"title":"About"},{"body":"TraduÃ§Ã£o em portuguÃªs. Apenas para demonstraÃ§Ã£o, o resto do artigo nÃ£o estÃ¡ traduzido.\nWritten in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\nhttps://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","link":"https://neilarmitage.com/about.pt/","section":"","tags":null,"title":"Sobre"},{"body":"","link":"https://neilarmitage.com/series/","section":"series","tags":null,"title":"Series"}]