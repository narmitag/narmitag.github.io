➜  eks git:(main) ✗ (⎈ |arn:aws:eks:eu-west-2:973101223973:cluster/forgerock-test:26597-policy-creation-afer-registration) export CLUSTER_NAME="karpenter-demo"
export AWS_DEFAULT_REGION="eu-west-1"
export AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
➜  eks git:(main) ✗ (⎈ |arn:aws:eks:eu-west-2:973101223973:cluster/forgerock-test:26597-policy-creation-afer-registration) eksctl create cluster -f - << EOF
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_DEFAULT_REGION}
  version: "1.21"
  tags:
    karpenter.sh/discovery: ${CLUSTER_NAME}
managedNodeGroups:
  - instanceType: t3.small
    name: ${CLUSTER_NAME}-ng
    desiredCapacity: 1
    minSize: 1
    maxSize: 2
iam:
  withOIDC: true
EOF
2022-05-13 11:02:23 [ℹ]  eksctl version 0.96.0
2022-05-13 11:02:23 [ℹ]  using region eu-west-1
2022-05-13 11:02:24 [ℹ]  setting availability zones to [eu-west-1c eu-west-1a eu-west-1b]
2022-05-13 11:02:24 [ℹ]  subnets for eu-west-1c - public:192.168.0.0/19 private:192.168.96.0/19
2022-05-13 11:02:24 [ℹ]  subnets for eu-west-1a - public:192.168.32.0/19 private:192.168.128.0/19
2022-05-13 11:02:24 [ℹ]  subnets for eu-west-1b - public:192.168.64.0/19 private:192.168.160.0/19
2022-05-13 11:02:24 [ℹ]  nodegroup "karpenter-demo-ng" will use "" [AmazonLinux2/1.21]
2022-05-13 11:02:24 [ℹ]  using Kubernetes version 1.21
2022-05-13 11:02:24 [ℹ]  creating EKS cluster "karpenter-demo" in "eu-west-1" region with managed nodes
2022-05-13 11:02:24 [ℹ]  1 nodegroup (karpenter-demo-ng) was included (based on the include/exclude rules)
2022-05-13 11:02:24 [ℹ]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)
2022-05-13 11:02:24 [ℹ]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)
2022-05-13 11:02:24 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=eu-west-1 --cluster=karpenter-demo'
2022-05-13 11:02:24 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "karpenter-demo" in "eu-west-1"
2022-05-13 11:02:24 [ℹ]  CloudWatch logging will not be enabled for cluster "karpenter-demo" in "eu-west-1"
2022-05-13 11:02:24 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=eu-west-1 --cluster=karpenter-demo'
2022-05-13 11:02:24 [ℹ]  
2 sequential tasks: { create cluster control plane "karpenter-demo", 
    2 sequential sub-tasks: { 
        4 sequential sub-tasks: { 
            wait for control plane to become ready,
            associate IAM OIDC provider,
            2 sequential sub-tasks: { 
                create IAM role for serviceaccount "kube-system/aws-node",
                create serviceaccount "kube-system/aws-node",
            },
            restart daemonset "kube-system/aws-node",
        },
        create managed nodegroup "karpenter-demo-ng",
    } 
}
2022-05-13 11:02:24 [ℹ]  building cluster stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:02:24 [ℹ]  deploying stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:02:54 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:03:24 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:04:24 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:05:25 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:06:25 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:07:25 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:08:25 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:09:26 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:10:26 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:11:26 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:12:26 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:13:27 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-cluster"
2022-05-13 11:15:29 [ℹ]  building iamserviceaccount stack "eksctl-karpenter-demo-addon-iamserviceaccount-kube-system-aws-node"
2022-05-13 11:15:30 [ℹ]  deploying stack "eksctl-karpenter-demo-addon-iamserviceaccount-kube-system-aws-node"
2022-05-13 11:15:30 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-addon-iamserviceaccount-kube-system-aws-node"
2022-05-13 11:16:00 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-addon-iamserviceaccount-kube-system-aws-node"
2022-05-13 11:16:00 [ℹ]  serviceaccount "kube-system/aws-node" already exists
2022-05-13 11:16:00 [ℹ]  updated serviceaccount "kube-system/aws-node"
2022-05-13 11:16:00 [ℹ]  daemonset "kube-system/aws-node" restarted
2022-05-13 11:16:01 [ℹ]  building managed nodegroup stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:16:01 [ℹ]  deploying stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:16:01 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:16:31 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:17:02 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:18:28 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:20:01 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-nodegroup-karpenter-demo-ng"
2022-05-13 11:20:01 [ℹ]  waiting for the control plane availability...
2022-05-13 11:20:02 [✔]  saved kubeconfig as "/Users/neilarmitage/.kube/config"
2022-05-13 11:20:02 [ℹ]  no tasks
2022-05-13 11:20:02 [✔]  all EKS cluster resources for "karpenter-demo" have been created
2022-05-13 11:20:02 [ℹ]  nodegroup "karpenter-demo-ng" has 1 node(s)
2022-05-13 11:20:02 [ℹ]  node "ip-192-168-8-14.eu-west-1.compute.internal" is ready
2022-05-13 11:20:02 [ℹ]  waiting for at least 1 node(s) to become ready in "karpenter-demo-ng"
2022-05-13 11:20:02 [ℹ]  nodegroup "karpenter-demo-ng" has 1 node(s)
2022-05-13 11:20:02 [ℹ]  node "ip-192-168-8-14.eu-west-1.compute.internal" is ready
2022-05-13 11:20:02 [ℹ]  kubectl command should work with "/Users/neilarmitage/.kube/config", try 'kubectl get nodes'
2022-05-13 11:20:02 [✔]  EKS cluster "karpenter-demo" in "eu-west-1" region is ready
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) k get no
NAME                                           STATUS   ROLES    AGE    VERSION
ip-192-168-16-22.eu-west-2.compute.internal    Ready    <none>   8d     v1.21.2-eks-55daa9d
ip-192-168-18-162.eu-west-2.compute.internal   Ready    <none>   6h3m   v1.21.2-eks-55daa9d
ip-192-168-20-238.eu-west-2.compute.internal   Ready    <none>   17h    v1.21.2-eks-55daa9d
ip-192-168-31-66.eu-west-2.compute.internal    Ready    <none>   9d     v1.21.2-eks-55daa9d
ip-192-168-41-235.eu-west-2.compute.internal   Ready    <none>   3h     v1.21.2-eks-55daa9d
ip-192-168-45-33.eu-west-2.compute.internal    Ready    <none>   136m   v1.21.2-eks-55daa9d
ip-192-168-49-158.eu-west-2.compute.internal   Ready    <none>   11h    v1.21.2-eks-55daa9d
ip-192-168-59-185.eu-west-2.compute.internal   Ready    <none>   17d    v1.21.2-eks-55daa9d
ip-192-168-60-124.eu-west-2.compute.internal   Ready    <none>   168m   v1.21.2-eks-55daa9d
ip-192-168-68-204.eu-west-2.compute.internal   Ready    <none>   7d5h   v1.21.2-eks-55daa9d
ip-192-168-71-119.eu-west-2.compute.internal   Ready    <none>   24h    v1.21.2-eks-55daa9d
ip-192-168-76-15.eu-west-2.compute.internal    Ready    <none>   37h    v1.21.2-eks-55daa9d
ip-192-168-82-55.eu-west-2.compute.internal    Ready    <none>   16d    v1.21.2-eks-55daa9d
ip-192-168-9-176.eu-west-2.compute.internal    Ready    <none>   13h    v1.21.2-eks-55daa9d
ip-192-168-93-178.eu-west-2.compute.internal   Ready    <none>   24h    v1.21.2-eks-55daa9d
➜  eks git:(main) ✗ (⎈ |arn:aws:eks:eu-west-2:973101223973:cluster/forgerock-test:fr-ci-3) ktx karp
Switched to context "neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io".
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) k get no
NAME                                         STATUS   ROLES    AGE     VERSION
ip-192-168-8-14.eu-west-1.compute.internal   Ready    <none>   3m34s   v1.21.5-eks-9017834
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) export CLUSTER_ENDPOINT="$(aws eks describe-cluster --name ${CLUSTER_NAME} --query "cluster.endpoint" --output text)"
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) TEMPOUT=$(mktemp)
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) curl -fsSL https://karpenter.sh/v0.6.3/getting-started/cloudformation.yaml  > $TEMPOUT \
&& aws cloudformation deploy \
  --stack-name "Karpenter-${CLUSTER_NAME}" \
  --template-file "${TEMPOUT}" \
  --capabilities CAPABILITY_NAMED_IAM \
  --parameter-overrides "ClusterName=${CLUSTER_NAME}"

Waiting for changeset to be created..
Waiting for stack create/update to complete
Successfully created/updated stack - Karpenter-karpenter-demo
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) eksctl create iamidentitymapping \
  --username system:node:{{EC2PrivateDNSName}} \
  --cluster "${CLUSTER_NAME}" \
  --arn "arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}" \
  --group system:bootstrappers \
  --group system:nodes
2022-05-13 11:25:23 [ℹ]  eksctl version 0.96.0
2022-05-13 11:25:23 [ℹ]  using region eu-west-1
2022-05-13 11:25:23 [ℹ]  adding identity "arn:aws:iam::569221643395:role/KarpenterNodeRole-karpenter-demo" to auth ConfigMap
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default)   eksctl create iamserviceaccount \
  --cluster "${CLUSTER_NAME}" --name karpenter --namespace karpenter \
  --role-name "${CLUSTER_NAME}-karpenter" \
  --attach-policy-arn "arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}" \
  --role-only \
  --approve
2022-05-13 11:25:30 [ℹ]  eksctl version 0.96.0
2022-05-13 11:25:30 [ℹ]  using region eu-west-1
2022-05-13 11:25:31 [ℹ]  1 existing iamserviceaccount(s) (kube-system/aws-node) will be excluded
2022-05-13 11:25:31 [ℹ]  1 iamserviceaccount (karpenter/karpenter) was included (based on the include/exclude rules)
2022-05-13 11:25:31 [!]  serviceaccounts that exist in Kubernetes will be excluded, use --override-existing-serviceaccounts to override
2022-05-13 11:25:31 [ℹ]  1 task: { create IAM role for serviceaccount "karpenter/karpenter" }
2022-05-13 11:25:31 [ℹ]  building iamserviceaccount stack "eksctl-karpenter-demo-addon-iamserviceaccount-karpenter-karpenter"
2022-05-13 11:25:31 [ℹ]  deploying stack "eksctl-karpenter-demo-addon-iamserviceaccount-karpenter-karpenter"
2022-05-13 11:25:31 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-addon-iamserviceaccount-karpenter-karpenter"
2022-05-13 11:26:02 [ℹ]  waiting for CloudFormation stack "eksctl-karpenter-demo-addon-iamserviceaccount-karpenter-karpenter"
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) export KARPENTER_IAM_ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter"

aws iam create-service-linked-role --aws-service-name spot.amazonaws.com

An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) helm repo add karpenter https://charts.karpenter.sh/
helm repo update

Error: repository name (karpenter) already exists, please specify a different name
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "secrets-store-csi-driver" chart repository
...Successfully got an update from the "karpenter" chart repository
...Successfully got an update from the "vmware-tanzu" chart repository
...Successfully got an update from the "cilium" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "projectcalico" chart repository
Update Complete. ⎈Happy Helming!⎈
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) helm upgrade --install --namespace karpenter --create-namespace \
  karpenter karpenter/karpenter \
  --version v0.6.3 \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=${KARPENTER_IAM_ROLE_ARN} \
  --set clusterName=${CLUSTER_NAME} \
  --set clusterEndpoint=${CLUSTER_ENDPOINT} \
  --set aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \
  --wait # for the defaulting webhook to install before creating a Provisioner
Release "karpenter" does not exist. Installing it now.
NAME: karpenter
LAST DEPLOYED: Fri May 13 11:26:38 2022
NAMESPACE: karpenter
STATUS: deployed
REVISION: 1
TEST SUITE: None
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) cat <<EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["spot"]
  limits:
    resources:
      cpu: 1000
  provider:
    subnetSelector:
      karpenter.sh/discovery: ${CLUSTER_NAME}
    securityGroupSelector:
      karpenter.sh/discovery: ${CLUSTER_NAME}
  ttlSecondsAfterEmpty: 30
EOF

provisioner.karpenter.sh/default created
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      terminationGracePeriodSeconds: 0
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
          resources:
            requests:
              cpu: 1
EOF
kubectl scale deployment inflate --replicas 5
deployment.apps/inflate created
deployment.apps/inflate scaled
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2022-05-13T10:26:47.308Z        INFO    controller.controller.nodemetrics       Starting workers        {"commit": "fd19ba2", "reconciler group": "", "reconciler kind": "Node", "worker count": 1}
2022-05-13T10:26:47.308Z        INFO    controller.controller.provisioning      Starting workers        {"commit": "fd19ba2", "reconciler group": "karpenter.sh", "reconciler kind": "Provisioner", "worker count": 10}
2022-05-13T10:26:59.993Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:22.951Z        INFO    controller.provisioning Batched 5 pods in 1.176501626s  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:23.156Z        INFO    controller.provisioning Computed packing of 1 node(s) for 5 pod(s) with instance type option(s) [c1.xlarge c4.2xlarge c3.2xlarge c5d.2xlarge c6i.2xlarge c5a.2xlarge c5.2xlarge c6a.2xlarge c5ad.2xlarge c5n.2xlarge m3.2xlarge m5ad.2xlarge t3.2xlarge m5d.2xlarge m5.2xlarge m5zn.2xlarge m5n.2xlarge t3a.2xlarge m5dn.2xlarge m4.2xlarge] {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.448Z        INFO    controller.provisioning Launched instance: i-0addfe38342f0960a, hostname: ip-192-168-111-106.eu-west-1.compute.internal, type: t3.2xlarge, zone: eu-west-1c, capacityType: spot {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.515Z        INFO    controller.provisioning Bound 5 pod(s) to node ip-192-168-111-106.eu-west-1.compute.internal    {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.515Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:26.517Z        INFO    controller.provisioning Batched 1 pods in 1.00078258s   {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:26.519Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
^C
➜  eks git:(main) ✗ (⎈ |neil.armitage@amido.com@karpenter-demo.eu-west-1.eksctl.io:default) kubectl delete deployment inflate
kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
deployment.apps "inflate" deleted
2022-05-13T10:26:47.308Z        INFO    controller.controller.provisioning      Starting workers        {"commit": "fd19ba2", "reconciler group": "karpenter.sh", "reconciler kind": "Provisioner", "worker count": 10}
2022-05-13T10:26:59.993Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:22.951Z        INFO    controller.provisioning Batched 5 pods in 1.176501626s  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:23.156Z        INFO    controller.provisioning Computed packing of 1 node(s) for 5 pod(s) with instance type option(s) [c1.xlarge c4.2xlarge c3.2xlarge c5d.2xlarge c6i.2xlarge c5a.2xlarge c5.2xlarge c6a.2xlarge c5ad.2xlarge c5n.2xlarge m3.2xlarge m5ad.2xlarge t3.2xlarge m5d.2xlarge m5.2xlarge m5zn.2xlarge m5n.2xlarge t3a.2xlarge m5dn.2xlarge m4.2xlarge] {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.448Z        INFO    controller.provisioning Launched instance: i-0addfe38342f0960a, hostname: ip-192-168-111-106.eu-west-1.compute.internal, type: t3.2xlarge, zone: eu-west-1c, capacityType: spot {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.515Z        INFO    controller.provisioning Bound 5 pod(s) to node ip-192-168-111-106.eu-west-1.compute.internal    {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:25.515Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:26.517Z        INFO    controller.provisioning Batched 1 pods in 1.00078258s   {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:28:26.519Z        INFO    controller.provisioning Waiting for unschedulable pods  {"commit": "fd19ba2", "provisioner": "default"}
2022-05-13T10:31:51.183Z        INFO    controller.node Added TTL to empty node {"commit": "fd19ba2", "node": "ip-192-168-111-106.eu-west-1.compute.internal"}
2022-05-13T10:32:21.001Z        INFO    controller.node Triggering termination after 30s for empty node {"commit": "fd19ba2", "node": "ip-192-168-111-106.eu-west-1.compute.internal"}
2022-05-13T10:32:21.031Z        INFO    controller.termination  Cordoned node   {"commit": "fd19ba2", "node": "ip-192-168-111-106.eu-west-1.compute.internal"}
2022-05-13T10:32:21.239Z        INFO    controller.termination  Deleted node    {"commit": "fd19ba2", "node": "ip-192-168-111-106.eu-west-1.compute.internal"}